{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Extraction (bucketed)\n",
    "\n",
    "Bucketed, resumable Tier-3 full-text extraction with the schema/prompt and SIMBAD resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Setup: imports, paths, OpenAI client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdcd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "from itertools import islice\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# Repo / IO paths (public)\n",
    "# -----------------------\n",
    "# If running in a notebook under repo/notebooks/, this resolves to repo root.\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "if (REPO_ROOT / \"src\").exists() and (REPO_ROOT / \"config\").exists():\n",
    "    pass\n",
    "else:\n",
    "    # fallback: assume notebook is in repo/notebooks/\n",
    "    REPO_ROOT = REPO_ROOT.parents[0]\n",
    "\n",
    "DATA_DIR = Path(os.environ.get(\"ASTRO_DATA_DIR\", str(REPO_ROOT / \"data\")))\n",
    "OUT_ROOT = Path(os.environ.get(\"ASTRO_OUT_DIR\", str(REPO_ROOT / \"outputs\")))\n",
    "\n",
    "PROMPT_VERSION = os.environ.get(\"PROMPT_VERSION\", \"full_extraction\")\n",
    "SCHEMA_VERSION = os.environ.get(\"SCHEMA_VERSION\", \"2026-01-18\")\n",
    "\n",
    "RUN_DIR = OUT_ROOT / \"full_extraction\"\n",
    "REQ_DIR = RUN_DIR / \"requests\"\n",
    "OUT_DIR = RUN_DIR / \"outputs\"\n",
    "USAGE_DIR = RUN_DIR / \"usage\"\n",
    "CACHE_DIR = RUN_DIR / \"cache\"\n",
    "\n",
    "for d in (REQ_DIR, OUT_DIR, USAGE_DIR, CACHE_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Upstream inputs (private)\n",
    "# -----------------------\n",
    "# These are not included in the public release. Users must provide paths if they\n",
    "# want to run extraction end-to-end.\n",
    "\n",
    "KG_ROOT = Path(os.environ.get(\"ASTRO_KG_ROOT\", str(REPO_ROOT / \"data\")))\n",
    "SUMMARIES_PATH = Path(os.environ.get(\"ASTRO_SUMMARIES_PATH\", str(KG_ROOT / \"papers_summaries.jsonl\")))\n",
    "ABSTRACTS_PATH = Path(os.environ.get(\"ASTRO_ABSTRACTS_PATH\", str(KG_ROOT / \"abstracts_all.jsonl\")))\n",
    "\n",
    "# Full OCR repository (private; not distributed). If absent, extraction cannot run,\n",
    "# but downstream post-processing can still run if bucket outputs exist.\n",
    "OCR_DIR = Path(os.environ.get(\"ASTRO_OCR_DIR\", \"\"))  # set to enable OCR-based extraction\n",
    "OCR_PATHS = []\n",
    "if str(OCR_DIR).strip():\n",
    "    OCR_DIR = OCR_DIR.expanduser().resolve()\n",
    "    if OCR_DIR.exists():\n",
    "        OCR_PATHS = sorted(OCR_DIR.glob(\"*.jsonl\"))\n",
    "    else:\n",
    "        print(f\"[WARN] OCR_DIR was set but does not exist: {OCR_DIR}\")\n",
    "else:\n",
    "    print(\"[INFO] OCR_DIR not set. Skipping OCR discovery (expected for public release).\")\n",
    "\n",
    "# -----------------------\n",
    "# Extraction controls\n",
    "# -----------------------\n",
    "MODEL = os.environ.get(\"ASTRO_LLM_MODEL\", \"gpt-5-mini\")  # logical model name\n",
    "MODEL_DEPLOYMENT = os.environ.get(\"ASTRO_LLM_DEPLOYMENT\", MODEL)  # Azure/OpenAI deployment name if applicable\n",
    "\n",
    "MAX_OCR_CHARS = int(os.environ.get(\"MAX_OCR_CHARS\", \"583000\"))      # cap to keep prompt size manageable\n",
    "MAX_EVIDENCE_CHARS = int(os.environ.get(\"MAX_EVIDENCE_CHARS\", \"300\"))\n",
    "\n",
    "# -----------------------\n",
    "# Optional OpenAI/Azure client\n",
    "# -----------------------\n",
    "client = None\n",
    "endpoint = os.environ.get(\"ASTRO_OPENAI_ENDPOINT\", \"\")  # e.g., https://<resource>.openai.azure.com/openai/v1\n",
    "api_key = os.environ.get(\"ASTRO_OPENAI_API_KEY\", \"\")    # or ASTROMLAB_API_KEY; keep generic for release\n",
    "\n",
    "if OpenAI is None:\n",
    "    print(\"[INFO] openai package not available. Install `openai` to run extraction calls.\")\n",
    "elif endpoint and api_key:\n",
    "    client = OpenAI(base_url=endpoint, api_key=api_key)\n",
    "    print(\"[INFO] OpenAI client initialized from ASTRO_OPENAI_ENDPOINT / ASTRO_OPENAI_API_KEY.\")\n",
    "else:\n",
    "    print(\"[INFO] OpenAI client not initialized. Set ASTRO_OPENAI_ENDPOINT and ASTRO_OPENAI_API_KEY to enable extraction calls.\")\n",
    "\n",
    "print(\"REPO_ROOT :\", REPO_ROOT)\n",
    "print(\"DATA_DIR  :\", DATA_DIR)\n",
    "print(\"OUT_ROOT  :\", OUT_ROOT)\n",
    "print(\"RUN_DIR   :\", RUN_DIR)\n",
    "print(\"SUMMARIES :\", SUMMARIES_PATH, \"| exists =\", SUMMARIES_PATH.exists())\n",
    "print(\"ABSTRACTS :\", ABSTRACTS_PATH, \"| exists =\", ABSTRACTS_PATH.exists())\n",
    "print(\"OCR_DIR   :\", OCR_DIR if str(OCR_DIR).strip() else \"<unset>\")\n",
    "print(\"OCR files :\", len(OCR_PATHS))\n",
    "print(\"MODEL     :\", MODEL, \"| DEPLOYMENT :\", MODEL_DEPLOYMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e15243",
   "metadata": {},
   "source": [
    "## 1) Load paper metadata (title/abstract/summary) and build id_to_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading summaries and abstracts...\n",
      "Usable papers with some text: 408590\n",
      "Unique arxiv_ids in metadata: 408590\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>abstract_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0007</td>\n",
       "      <td>Polymer Quantum Mechanics and its Continuum Limit</td>\n",
       "      <td>Polymer quantum mechanics presents an unconven...</td>\n",
       "      <td>A rather non-standard quantum representation o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0009</td>\n",
       "      <td>The Spitzer c2d Survey of Large, Nearby, Inter...</td>\n",
       "      <td>The Serpens star-forming cloud is one of five ...</td>\n",
       "      <td>We discuss the results from the combined IRAC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0017</td>\n",
       "      <td>Spectroscopic Observations of the Intermediate...</td>\n",
       "      <td>EX Hydrae (EX Hya) is classified as an Interme...</td>\n",
       "      <td>Results from spectroscopic observations of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    arxiv_id                                              title  \\\n",
       "0  0704.0007  Polymer Quantum Mechanics and its Continuum Limit   \n",
       "1  0704.0009  The Spitzer c2d Survey of Large, Nearby, Inter...   \n",
       "2  0704.0017  Spectroscopic Observations of the Intermediate...   \n",
       "\n",
       "                                        summary_text  \\\n",
       "0  Polymer quantum mechanics presents an unconven...   \n",
       "1  The Serpens star-forming cloud is one of five ...   \n",
       "2  EX Hydrae (EX Hya) is classified as an Interme...   \n",
       "\n",
       "                                       abstract_text  \n",
       "0  A rather non-standard quantum representation o...  \n",
       "1  We discuss the results from the combined IRAC ...  \n",
       "2  Results from spectroscopic observations of the...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Load paper metadata (title/abstract/summary) and build id_to_meta (release-safe)\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "def _warn_missing(path: Path, name: str):\n",
    "    print(f\"[WARN] {name} not found: {path}\")\n",
    "    print(\"       This notebook can still run post-processing steps if you already have bucket outputs,\")\n",
    "    print(\"       but end-to-end extraction requires these metadata files.\")\n",
    "    return {}\n",
    "\n",
    "# In the public release, these upstream metadata files may not exist.\n",
    "# We therefore degrade gracefully rather than asserting.\n",
    "print(\"Loading summaries and abstracts...\")\n",
    "\n",
    "summaries_by_arxiv = {}\n",
    "abstracts_by_arxiv = {}\n",
    "\n",
    "if SUMMARIES_PATH.exists():\n",
    "    summaries_by_arxiv = {row.get(\"arxiv_id\"): row.get(\"summary\") for row in read_jsonl(SUMMARIES_PATH) if row.get(\"arxiv_id\")}\n",
    "else:\n",
    "    _warn_missing(SUMMARIES_PATH, \"Summaries file\")\n",
    "\n",
    "if ABSTRACTS_PATH.exists():\n",
    "    abstracts_by_arxiv = {row.get(\"arxiv_id\"): (row.get(\"abstract\", \"\") or \"\") for row in read_jsonl(ABSTRACTS_PATH) if row.get(\"arxiv_id\")}\n",
    "else:\n",
    "    _warn_missing(ABSTRACTS_PATH, \"Abstracts file\")\n",
    "\n",
    "rows = []\n",
    "for aid, summary in summaries_by_arxiv.items():\n",
    "    if not aid:\n",
    "        continue\n",
    "\n",
    "    s_dict = summary or {}\n",
    "    title_and_author = s_dict.get(\"title_and_author\", \"\") or \"\"\n",
    "\n",
    "    title = \"\"\n",
    "    if title_and_author:\n",
    "        lines = [ln.strip() for ln in title_and_author.splitlines() if ln.strip()]\n",
    "        if lines:\n",
    "            title = lines[0].strip(\"* \").strip()\n",
    "\n",
    "    summary_text_parts = []\n",
    "    for k, v in s_dict.items():\n",
    "        if k == \"title_and_author\":\n",
    "            continue\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            summary_text_parts.append(v.strip())\n",
    "    summary_text = \"\\n\\n\".join(summary_text_parts)\n",
    "\n",
    "    abstract_text = abstracts_by_arxiv.get(aid, \"\")\n",
    "\n",
    "    if not (title.strip() or summary_text.strip() or abstract_text.strip()):\n",
    "        continue\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"arxiv_id\": aid,\n",
    "            \"title\": title,\n",
    "            \"summary_text\": summary_text,\n",
    "            \"abstract_text\": abstract_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_papers_df = pd.DataFrame(rows)\n",
    "\n",
    "id_to_meta = {\n",
    "    row[\"arxiv_id\"]: {\n",
    "        \"arxiv_id\": row[\"arxiv_id\"],\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"summary_text\": row.get(\"summary_text\", \"\"),\n",
    "        \"abstract_text\": row.get(\"abstract_text\", \"\"),\n",
    "    }\n",
    "    for _, row in all_papers_df.iterrows()\n",
    "}\n",
    "\n",
    "ALL_ARXIV_IDS = sorted(id_to_meta.keys())\n",
    "\n",
    "print(\"Usable papers with some text:\", len(all_papers_df))\n",
    "print(\"Unique arxiv_ids in metadata:\", len(ALL_ARXIV_IDS))\n",
    "\n",
    "try:\n",
    "    display(all_papers_df.head(3))\n",
    "except Exception:\n",
    "    print(all_papers_df.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf96a5",
   "metadata": {},
   "source": [
    "## 2) Define schema + prompt builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e13f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_MODES = {\n",
    "    \"new_observation\",         # authors report newly obtained observations of this object\n",
    "    \"archival_or_reanalysis\",  # reanalysis of previously obtained/public data\n",
    "    \"catalog_compilation\",     # primarily catalog/survey DB values + crossmatch/statistical compilation\n",
    "    \"simulation_or_theory\",    # modeling/simulations/analytic theory; no direct observational analysis of this object here\n",
    "    \"not_applicable\",          # object is mentioned only as comparison/calibration/incidental; not analyzed as data subject\n",
    "    \"unknown\",                 # truly ambiguous from provided text\n",
    "}\n",
    "\n",
    "STUDY_MODE_DEFINITIONS: dict[str, str] = {\n",
    "    \"new_observation\": \"New observations of this object are presented in this paper (authors obtained new data).\",\n",
    "    \"archival_or_reanalysis\": \"This paper analyzes previously obtained/public data for this object (archive/reanalysis).\",\n",
    "    \"catalog_compilation\": \"This paper primarily uses catalog/survey database values for this object (crossmatch/compilation/statistics).\",\n",
    "    \"simulation_or_theory\": \"This paper discusses this object only in theory/simulation/analytic modeling (no direct observational analysis here).\",\n",
    "    \"not_applicable\": \"Object is only referenced for comparison/calibration/incidental context; not an analyzed data subject here.\",\n",
    "    \"unknown\": \"Cannot determine from the provided text.\",\n",
    "}\n",
    "\n",
    "STUDY_MODE_PRIORITY = [\n",
    "    \"new_observation\",\n",
    "    \"archival_or_reanalysis\",\n",
    "    \"catalog_compilation\",\n",
    "    \"simulation_or_theory\",\n",
    "    \"not_applicable\",\n",
    "    \"unknown\",\n",
    "]\n",
    "\n",
    "study_mode_lines = \"\\n\".join(f\"- {m}: {STUDY_MODE_DEFINITIONS[m]}\" for m in STUDY_MODE_PRIORITY)\n",
    "study_mode_enum = \", \".join([f'\"{m}\"' for m in STUDY_MODE_PRIORITY])\n",
    "study_mode_priority_str = \" > \".join(STUDY_MODE_PRIORITY)\n",
    "\n",
    "\n",
    "ROLES = {\n",
    "    \"primary_subject\",              # the object is a main focus of the paper's results\n",
    "    \"member_of_sample\",             # included in a study sample / catalog list used in analysis\n",
    "    \"host_or_counterpart\",          # established association / ID / host / counterpart\n",
    "    \"candidate_association\",        # tentative counterpart/host/ID (hedged)\n",
    "    \"foreground_or_lens\",           # confirmed foreground/lens\n",
    "    \"background_or_lensed_source\",  # (optional) explicitly the lensed/background source\n",
    "    \"comparison_or_reference\",      # used as benchmark / reference example (not a studied subject here)\n",
    "    \"calibration\",                  # used for PSF/flux/astrometric/telluric calibration\n",
    "    \"serendipitous_or_field_source\",# incidental neighbor/contaminant in field\n",
    "    \"non_detection_or_upper_limit\", # searched/targeted but non-detection/upper limit\n",
    "    \"other\",\n",
    "}\n",
    "\n",
    "\n",
    "ROLE_DEFINITIONS: dict[str, str] = {\n",
    "    # High-signal, primary edges\n",
    "    \"primary_subject\": (\n",
    "        \"Primary subject of the paper's results: the object the paper mainly analyzes or draws conclusions about \"\n",
    "        \"(observational, archival, catalog-based, or theoretical). Often central to the title/abstract/results.\"\n",
    "    ),\n",
    "\n",
    "    \"member_of_sample\": (\n",
    "        \"Object is part of a study sample/survey used for statistical analysis or population results \"\n",
    "        \"(often listed in tables). Not necessarily deeply analyzed individually.\"\n",
    "    ),\n",
    "\n",
    "    # Association edges\n",
    "    \"host_or_counterpart\": (\n",
    "        \"Confirmed/established physical association or identification: host galaxy, optical/IR/radio counterpart, \"\n",
    "        \"companion star, etc. Language implies it is known/accepted in the paper.\"\n",
    "    ),\n",
    "    \"candidate_association\": (\n",
    "        \"Proposed/tentative association or identification: candidate counterpart/host/lens/ID. \"\n",
    "        \"Use when paper hedges (possible/likely/may be/consistent with) or not confirmed.\"\n",
    "    ),\n",
    "\n",
    "    # Lensing edges (optional split)\n",
    "    \"foreground_or_lens\": (\n",
    "        \"Foreground object affecting the observation, especially a confirmed gravitational lens or foreground star/galaxy.\"\n",
    "    ),\n",
    "    \"background_or_lensed_source\": (\n",
    "        \"Background source being lensed/magnified (the lensed object), if explicitly discussed as the source behind the lens.\"\n",
    "    ),\n",
    "\n",
    "    # Context / methodology edges (lower signal)\n",
    "    \"comparison_or_reference\": (\n",
    "        \"Object used mainly as a comparison/benchmark/reference example (not a main analyzed target), \"\n",
    "        \"e.g., classic sources used to compare spectra/lightcurves.\"\n",
    "    ),\n",
    "    \"calibration\": (\n",
    "        \"Object used operationally to calibrate or characterize data (flux/PSF/astrometric/telluric standard, etc.).\"\n",
    "    ),\n",
    "    \"serendipitous_or_field_source\": (\n",
    "        \"Incidental object in the field/nearby/contaminating/blended/background source mentioned for context \"\n",
    "        \"rather than as a scientific target.\"\n",
    "    ),\n",
    "    \"non_detection_or_upper_limit\": (\n",
    "        \"Object was searched/targeted for a signal but the result is a non-detection or upper limit.\"\n",
    "    ),\n",
    "    \"other\": \"Does not fit above roles; use sparingly.\",\n",
    "}\n",
    "_missing_defs = sorted(ROLES - set(ROLE_DEFINITIONS))\n",
    "_extra_defs = sorted(set(ROLE_DEFINITIONS) - ROLES)\n",
    "if _missing_defs or _extra_defs:\n",
    "    raise ValueError(f\"ROLE_DEFINITIONS mismatch. missing={_missing_defs} extra={_extra_defs}\")\n",
    "ROLE_PRIORITY = [\n",
    "    \"non_detection_or_upper_limit\",\n",
    "    \"primary_subject\",\n",
    "    \"member_of_sample\",\n",
    "\n",
    "    \"host_or_counterpart\",\n",
    "    \"candidate_association\",\n",
    "    \"foreground_or_lens\",\n",
    "    \"background_or_lensed_source\",\n",
    "\n",
    "    \"calibration\",\n",
    "    \"serendipitous_or_field_source\",\n",
    "    \"comparison_or_reference\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "role_lines = \"\\n\".join(\n",
    "    f\"- {r}: {ROLE_DEFINITIONS[r]}\"\n",
    "    for r in ROLE_PRIORITY\n",
    "    if r in ROLES\n",
    ")\n",
    "role_enum = \", \".join([f'\"{r}\"' for r in ROLE_PRIORITY if r in ROLES])\n",
    "priority_str = \" > \".join([r for r in ROLE_PRIORITY if r in ROLES])\n",
    "EVIDENCE_SOURCES = {\"title\", \"abstract\", \"body\"}\n",
    "\n",
    "# 2) Define schema + prompt builder (ASTRO OBJECTS ONLY, simplified)\n",
    "SCHEMA_OBJ = {\n",
    "  \"type\": \"object\",\n",
    "  \"additionalProperties\": False,\n",
    "  \"required\": [\"arxiv_id\", \"objects\"],\n",
    "  \"properties\": {\n",
    "    \"arxiv_id\": {\"type\": \"string\"},\n",
    "    \"objects\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"additionalProperties\": False,\n",
    "        \"required\": [\"name\", \"role\", \"study_mode\", \"evidence_span\", \"evidence_source\"],\n",
    "        \"properties\": {\n",
    "          \"name\": {\"type\": \"string\"},\n",
    "          \"role\": {\"type\": \"string\", \"enum\": sorted(ROLES)},\n",
    "          \"study_mode\": {\"type\": \"string\", \"enum\": sorted(STUDY_MODES)},\n",
    "          \"evidence_span\": {\"type\": \"string\"},\n",
    "          \"evidence_source\": {\"type\": \"string\", \"enum\": sorted(EVIDENCE_SOURCES)},\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "SCHEMA_JSON = json.dumps(SCHEMA_OBJ, indent=2)\n",
    "SCHEMA_DOC = SCHEMA_JSON\n",
    "\n",
    "\n",
    "# ---- helpers ----\n",
    "FORBIDDEN_NAME_CHARS = [\",\", \"/\", \"=\", \";\", \"(\", \")\", \"[\", \"]\"]\n",
    "\n",
    "def _check_enum(val, allowed: set[str], field: str, errors: list[str]):\n",
    "    if val not in allowed:\n",
    "        errors.append(f\"{field} invalid: {val}\")\n",
    "\n",
    "def _violates_single_designation(name: str) -> bool:\n",
    "    # enforce “single designation only” cheaply\n",
    "    if any(ch in name for ch in FORBIDDEN_NAME_CHARS):\n",
    "        return True\n",
    "    # disallow obvious multi-name separators\n",
    "    if \" and \" in name.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_prompt_fulltext(meta: dict, full_text: str) -> tuple[str, str]:\n",
    "    raw_text = full_text or \"\"\n",
    "    truncated_text = raw_text if (MAX_OCR_CHARS is None or len(raw_text) <= MAX_OCR_CHARS) else raw_text[:MAX_OCR_CHARS]\n",
    "\n",
    "    system_msg = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        You are an expert astrophysicist helping to build a paper-object knowledge graph.\n",
    "\n",
    "        You will be given:\n",
    "        - Paper title\n",
    "        - Paper abstract\n",
    "        - Full OCR'd paper text (may be noisy / truncated)\n",
    "\n",
    "        TASK:\n",
    "        Extract ONLY named astrophysical objects intended for SIMBAD-style resolution, e.g.:\n",
    "        - Galaxies: NGC/IC/M/UGC/ESO/SDSS objects (e.g., \"NGC 1275\", \"M 31\", \"SDSS J1234+5678\")\n",
    "        - Stars: HD/HIP/TYC/2MASS/Gaia/variable-star designations, etc.\n",
    "        - Clusters / nebulae / SNR / AGN / radio sources / pulsars with standard designations (PSR J..., 3C..., etc.)\n",
    "\n",
    "        DO NOT include:\n",
    "        - Instruments, facilities, surveys, catalogs (e.g., ALMA, SDSS, Gaia)\n",
    "        - Sky regions / fields / pointings\n",
    "        - Solar system bodies or features (rings/craters/etc.)\n",
    "        - Generic classes (\"galaxies\", \"supernovae\") unless a specific named object is given\n",
    "        - Raw coordinates unless explicitly labeled as a named object\n",
    "\n",
    "        ROLE (choose exactly one):\n",
    "        Allowed roles (use these exact strings):\n",
    "        [{role_enum}]\n",
    "\n",
    "        ROLE DEFINITIONS:\n",
    "        {role_lines}\n",
    "\n",
    "        ROLE CHOICE RULES (if multiple could apply, choose the highest priority):\n",
    "        {priority_str}\n",
    "        Notes:\n",
    "        - If an object is a primary target BUT the paper reports only an upper limit/non-detection, use non_detection_or_upper_limit.\n",
    "        - Use candidate_association when the paper is uncertain (possible/likely/may be/consistent with), even if a counterpart name is given.\n",
    "        - Use calibration only for operational calibration/PSF/standards (not “we compare to Crab”).\n",
    "        - Use serendipitous_or_field_source for incidental neighbors/contaminants/blends.\n",
    "\n",
    "        STUDY MODE (choose exactly one per object):\n",
    "        Allowed study_mode values (use these exact strings):\n",
    "        [{study_mode_enum}]\n",
    "\n",
    "        STUDY MODE DEFINITIONS:\n",
    "        {study_mode_lines}\n",
    "\n",
    "        STUDY MODE CHOICE RULES (if multiple could apply, choose highest priority):\n",
    "        {study_mode_priority_str}\n",
    "        Notes:\n",
    "        - primary_subject / non_detection_or_upper_limit should almost never have study_mode=not_applicable.\n",
    "        - comparison_or_reference / calibration / serendipitous_or_field_source should usually have study_mode=not_applicable.\n",
    "        - Use unknown only if truly ambiguous from the provided text.\n",
    "\n",
    "\n",
    "\n",
    "        EVIDENCE:\n",
    "        For each object, include:\n",
    "        - name\n",
    "        - role\n",
    "        - study_mode\n",
    "        - evidence_span (<= {MAX_EVIDENCE_CHARS} chars)\n",
    "        - evidence_source\n",
    "\n",
    "\n",
    "        PRECISION RULES:\n",
    "        - Be conservative: omit passing mentions and long literature lists without new analysis.\n",
    "        - Prefer objects central to the paper (main targets, analyzed sample members, key counterparts, major comparison objects).\n",
    "        - Typical object count <= 25; hard cap at 40.\n",
    "        - Do not invent names. Use text as written (minor whitespace fixes OK).\n",
    "\n",
    "        SINGLE DESIGNATION ONLY:\n",
    "        - name MUST be exactly ONE designation string.\n",
    "        - name must NOT contain parentheses, commas, slashes, semicolons, '=' or multiple aliases joined together.\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        Return a single JSON object that strictly matches this schema (no extra keys, valid JSON):\n",
    "        {SCHEMA_DOC}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "    # Important: do NOT include prompt_version/schema_version here;\n",
    "    # the model may echo them into JSON as extra keys.\n",
    "    user_msg = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        Paper arXiv ID: {meta['arxiv_id']}\n",
    "\n",
    "        TITLE:\n",
    "        {meta.get('title', '').strip()}\n",
    "\n",
    "        ABSTRACT:\n",
    "        {meta.get('abstract_text', '').strip()}\n",
    "\n",
    "        FULL PAPER TEXT (OCR, may be truncated):\n",
    "        \\\"\\\"\\\"\n",
    "        {truncated_text}\n",
    "        \\\"\\\"\\\"\n",
    "\n",
    "        Return only the JSON object described above.\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "\n",
    "    return system_msg, user_msg\n",
    "\n",
    "\n",
    "def make_request(arxiv_id: str, sys_msg: str, user_msg: str) -> dict:\n",
    "    custom_id = f\"V2-{arxiv_id}\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": MODEL,\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": sys_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "        },\n",
    "        # keep versions here (safe), not in the prompt-visible JSON schema\n",
    "        \"metadata\": {\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"prompt_version\": PROMPT_VERSION,\n",
    "            \"schema_version\": SCHEMA_VERSION,\n",
    "            \"model\": MODEL,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_record(rec: dict) -> list[str]:\n",
    "    errors: list[str] = []\n",
    "    if not isinstance(rec, dict):\n",
    "        return [\"record is not an object\"]\n",
    "\n",
    "    # Require exact top-level keys (strict)\n",
    "    required_top = {\"arxiv_id\", \"objects\"}\n",
    "    for k in required_top:\n",
    "        if k not in rec:\n",
    "            errors.append(f\"missing key: {k}\")\n",
    "\n",
    "    extra_top = set(rec.keys()) - required_top\n",
    "    if extra_top:\n",
    "        errors.append(f\"extra top-level keys: {sorted(extra_top)}\")\n",
    "\n",
    "    arxiv_id = rec.get(\"arxiv_id\")\n",
    "    if not isinstance(arxiv_id, str) or not arxiv_id.strip():\n",
    "        errors.append(\"arxiv_id missing/empty\")\n",
    "\n",
    "    objects = rec.get(\"objects\")\n",
    "    if not isinstance(objects, list):\n",
    "        errors.append(\"objects is not a list\")\n",
    "        return errors\n",
    "\n",
    "    if len(objects) > 40:\n",
    "        errors.append(\"too many objects (>40)\")\n",
    "\n",
    "    for idx, obj in enumerate(objects):\n",
    "        prefix = f\"objects[{idx}]\"\n",
    "        if not isinstance(obj, dict):\n",
    "            errors.append(f\"{prefix} not an object\")\n",
    "            continue\n",
    "\n",
    "        required_obj = {\"name\", \"role\", \"study_mode\", \"evidence_span\", \"evidence_source\"}\n",
    "\n",
    "        for k in required_obj:\n",
    "            if k not in obj:\n",
    "                errors.append(f\"missing key: {prefix}.{k}\")\n",
    "        role = obj.get(\"role\")\n",
    "        mode = obj.get(\"study_mode\")\n",
    "\n",
    "        # Roles that are usually \"not_applicable\" (context-only)\n",
    "        context_roles = {\"comparison_or_reference\", \"calibration\", \"serendipitous_or_field_source\"}\n",
    "\n",
    "        # Roles that should almost never be \"not_applicable\"\n",
    "        substantive_roles = {\n",
    "            \"primary_subject\", \"member_of_sample\", \"host_or_counterpart\", \"candidate_association\",\n",
    "            \"foreground_or_lens\", \"background_or_lensed_source\", \"non_detection_or_upper_limit\"\n",
    "        }\n",
    "\n",
    "        if role in substantive_roles and mode == \"not_applicable\":\n",
    "            errors.append(f\"{prefix}.study_mode not_applicable inconsistent with role={role}\")\n",
    "\n",
    "        if role in context_roles and mode not in {\"not_applicable\", \"unknown\"}:\n",
    "            errors.append(f\"{prefix}.study_mode should usually be not_applicable for role={role} (got {mode})\")\n",
    "\n",
    "        if role == \"non_detection_or_upper_limit\" and mode == \"simulation_or_theory\":\n",
    "            errors.append(f\"{prefix}.study_mode simulation_or_theory inconsistent with non_detection_or_upper_limit\")\n",
    "\n",
    "\n",
    "        extra_obj = set(obj.keys()) - required_obj\n",
    "        if extra_obj:\n",
    "            errors.append(f\"extra keys in {prefix}: {sorted(extra_obj)}\")\n",
    "\n",
    "        name = obj.get(\"name\")\n",
    "        if not isinstance(name, str) or not name.strip():\n",
    "            errors.append(f\"{prefix}.name missing/empty\")\n",
    "        else:\n",
    "            if _violates_single_designation(name):\n",
    "                errors.append(f\"{prefix}.name violates single-designation rule\")\n",
    "\n",
    "        _check_enum(obj.get(\"role\"), ROLES, f\"{prefix}.role\", errors)\n",
    "        _check_enum(obj.get(\"study_mode\"), STUDY_MODES, f\"{prefix}.study_mode\", errors)\n",
    "\n",
    "\n",
    "        ev_span = obj.get(\"evidence_span\", \"\") or \"\"\n",
    "        if not isinstance(ev_span, str) or not ev_span.strip():\n",
    "            errors.append(f\"{prefix}.evidence_span missing/empty\")\n",
    "        elif len(ev_span) > MAX_EVIDENCE_CHARS:\n",
    "            errors.append(f\"{prefix}.evidence_span too long\")\n",
    "\n",
    "        _check_enum(obj.get(\"evidence_source\"), EVIDENCE_SOURCES, f\"{prefix}.evidence_source\", errors)\n",
    "\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899fffe6",
   "metadata": {},
   "source": [
    "## 3) Bucket plan (manifest) + done_id detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0507c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Bucket plan (manifest) + done_id detection \n",
    "#\n",
    "# Notes:\n",
    "# - This notebook documents the extraction workflow used to generate the released artifacts.\n",
    "# - This section supports resumable execution by detecting which arXiv IDs already have outputs.\n",
    "\n",
    "# Bucket controls (override via env without editing the notebook)\n",
    "BUCKET_SIZE = int(os.environ.get(\"BUCKET_SIZE\", \"5000\"))\n",
    "BUCKET_START = int(os.environ.get(\"BUCKET_START\", \"0\"))\n",
    "_bucket_end_raw = os.environ.get(\"BUCKET_END\", \"\").strip()\n",
    "BUCKET_END = int(_bucket_end_raw) if _bucket_end_raw else None  # set to an int to truncate\n",
    "\n",
    "selected_ids = ALL_ARXIV_IDS[BUCKET_START:BUCKET_END]\n",
    "buckets = [selected_ids[i : i + BUCKET_SIZE] for i in range(0, len(selected_ids), BUCKET_SIZE)]\n",
    "\n",
    "manifest_rows = []\n",
    "for idx, ids in enumerate(buckets):\n",
    "    bucket_id = f\"{idx:04d}\"\n",
    "    manifest_rows.append(\n",
    "        {\n",
    "            \"bucket_id\": bucket_id,\n",
    "            \"n_papers\": len(ids),\n",
    "            \"first_arxiv_id\": ids[0] if ids else None,\n",
    "            \"last_arxiv_id\": ids[-1] if ids else None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "bucket_manifest_path = RUN_DIR / \"bucket_manifest.csv\"\n",
    "pd.DataFrame(manifest_rows).to_csv(bucket_manifest_path, index=False)\n",
    "print(f\"Wrote bucket manifest with {len(manifest_rows)} buckets -> {bucket_manifest_path}\")\n",
    "\n",
    "bucket_index = {row[\"bucket_id\"]: buckets[idx] for idx, row in enumerate(manifest_rows)}\n",
    "\n",
    "\n",
    "def load_done_arxiv_ids(out_dir: Path) -> set[str]:\n",
    "    \"\"\"Collect arXiv IDs that already have a successful parsed record in bucket outputs.\"\"\"\n",
    "    if not out_dir.exists():\n",
    "        return set()\n",
    "\n",
    "    done = set()\n",
    "    for path in sorted(out_dir.glob(\"bucket_*_outputs.jsonl\")):\n",
    "        with path.open() as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if rec.get(\"error\"):\n",
    "                    continue\n",
    "                parsed = rec.get(\"parsed\")\n",
    "                if isinstance(parsed, dict):\n",
    "                    aid = (\n",
    "                        parsed.get(\"arxiv_id\")\n",
    "                        or rec.get(\"arxiv_id\")\n",
    "                        or (rec.get(\"metadata\") or {}).get(\"arxiv_id\")\n",
    "                    )\n",
    "                    if aid:\n",
    "                        done.add(aid)\n",
    "    return done\n",
    "\n",
    "DONE_ARXIV_IDS = load_done_arxiv_ids(OUT_DIR)\n",
    "print(\"Done arxiv_ids from existing outputs:\", len(DONE_ARXIV_IDS))\n",
    "print(\"Remaining arxiv_ids to process:\", len(set(ALL_ARXIV_IDS) - DONE_ARXIV_IDS))\n",
    "\n",
    "# For release: only skip what has already been completed in outputs.\n",
    "SKIP_ARXIV_IDS = DONE_ARXIV_IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596cbfb",
   "metadata": {},
   "source": [
    "## 4) Build requests for one bucket (stream OCR jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote bucket manifest with 82 buckets -> /Users/jinchuli/projects/astro-llm-tools/data/llm_object_extraction_final/full_extraction_v2/bucket_manifest.csv\n",
      "First bucket IDs: ['0000', '0001', '0002', '0003', '0004']\n"
     ]
    }
   ],
   "source": [
    "# After manifest_rows is built\n",
    "bucket_manifest = pd.DataFrame(manifest_rows)\n",
    "\n",
    "# Use RUN_DIR from release-safe setup\n",
    "bucket_manifest_path = RUN_DIR / \"bucket_manifest.csv\"\n",
    "bucket_manifest.to_csv(bucket_manifest_path, index=False)\n",
    "\n",
    "print(f\"Wrote bucket manifest with {len(bucket_manifest)} buckets -> {bucket_manifest_path}\")\n",
    "\n",
    "bucket_index = {row[\"bucket_id\"]: buckets[idx] for idx, row in enumerate(manifest_rows)}\n",
    "BUCKET_IDS = sorted(bucket_index.keys())\n",
    "\n",
    "print(\"First bucket IDs:\", BUCKET_IDS[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e61461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target buckets: ['0000', '0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0016', '0017', '0018', '0019', '0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029', '0030', '0031', '0032', '0033', '0034', '0035', '0036', '0037', '0038', '0039', '0040', '0041', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0049', '0050', '0051', '0052', '0053', '0054', '0055', '0056', '0057', '0058', '0059', '0060', '0061', '0062', '0063', '0064', '0065', '0066', '0067', '0068', '0069', '0070', '0071', '0072', '0073', '0074', '0075', '0076', '0077', '0078', '0079', '0080', '0081']\n"
     ]
    }
   ],
   "source": [
    "# 4) Build requests for one or more buckets (stream OCR jsonl) \n",
    "#\n",
    "# Notes:\n",
    "# - End-to-end request building requires private OCR inputs (OCR_DIR) and metadata (id_to_meta).\n",
    "# - In the public release, this cell is safe to import/inspect, but it will no-op unless OCR_DIR is set and exists.\n",
    "\n",
    "def build_requests_for_bucket(\n",
    "    bucket_id: str,\n",
    "    allowed_ids: set[str] | None = None,\n",
    "    skip_ids: set[str] | None = None,\n",
    ") -> set[str]:\n",
    "    if bucket_id not in bucket_index:\n",
    "        raise ValueError(f\"Unknown bucket_id: {bucket_id}\")\n",
    "\n",
    "    # OCR is not included in the public release; require OCR_DIR to be set and valid.\n",
    "    if not str(OCR_DIR).strip() or not Path(OCR_DIR).exists():\n",
    "        print(f\"[WARN] OCR_DIR is not set or does not exist. Cannot build requests for bucket {bucket_id}.\")\n",
    "        return set()\n",
    "\n",
    "    target_ids = set(bucket_index[bucket_id])\n",
    "    if allowed_ids:\n",
    "        target_ids &= set(allowed_ids)\n",
    "    if skip_ids:\n",
    "        target_ids -= set(skip_ids)\n",
    "    if not target_ids:\n",
    "        print(f\"No target_ids to build for bucket {bucket_id}\")\n",
    "        return set()\n",
    "\n",
    "    req_path = REQ_DIR / f\"bucket_{bucket_id}_requests.jsonl\"\n",
    "    req_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    missing_ids = set(target_ids)\n",
    "    found_ids = set()\n",
    "\n",
    "    with req_path.open(\"w\") as fout:\n",
    "        for ocr_path in sorted(Path(OCR_DIR).glob(\"*.jsonl\")):\n",
    "            if not missing_ids:\n",
    "                break\n",
    "\n",
    "            with ocr_path.open() as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    aid = obj.get(\"arxiv_id\")\n",
    "                    if aid not in missing_ids:\n",
    "                        continue\n",
    "\n",
    "                    meta = id_to_meta.get(aid)\n",
    "                    if not meta:\n",
    "                        # Missing metadata: skip (request would be low quality / incomplete)\n",
    "                        missing_ids.remove(aid)\n",
    "                        continue\n",
    "\n",
    "                    sys_msg, user_msg = build_prompt_fulltext(\n",
    "                        meta,\n",
    "                        obj.get(\"ocr_markdown\", \"\") or \"\",\n",
    "                    )\n",
    "                    req = make_request(aid, sys_msg=sys_msg, user_msg=user_msg)\n",
    "                    fout.write(json.dumps(req) + \"\\n\")\n",
    "\n",
    "                    found_ids.add(aid)\n",
    "                    missing_ids.remove(aid)\n",
    "\n",
    "    if missing_ids:\n",
    "        missing_path = RUN_DIR / f\"missing_ocr_bucket_{bucket_id}.txt\"\n",
    "        with missing_path.open(\"w\") as m:\n",
    "            for mid in sorted(missing_ids):\n",
    "                m.write(mid + \"\\n\")\n",
    "        print(f\"OCR missing for {len(missing_ids)} ids -> {missing_path}\")\n",
    "\n",
    "    print(f\"Wrote {len(found_ids)} requests to {req_path}\")\n",
    "    return found_ids\n",
    "\n",
    "\n",
    "# ---- SAFE controls (default does nothing unless explicitly enabled) ----\n",
    "# Use env overrides so users don't need to edit the notebook.\n",
    "TARGET_BUCKET_ID = os.environ.get(\"TARGET_BUCKET_ID\", \"\").strip()       # e.g. \"0\" or \"0000\"\n",
    "TARGET_BUCKET_IDS = os.environ.get(\"TARGET_BUCKET_IDS\", \"\").strip()     # e.g. \"0000,0001\"\n",
    "BUILD_ALL_BUCKETS = bool(int(os.environ.get(\"BUILD_ALL_BUCKETS\", \"0\"))) # default OFF in release\n",
    "\n",
    "if TARGET_BUCKET_IDS:\n",
    "    target_bucket_ids = [b.strip().zfill(4) for b in TARGET_BUCKET_IDS.split(\",\") if b.strip()]\n",
    "elif TARGET_BUCKET_ID:\n",
    "    target_bucket_ids = [str(TARGET_BUCKET_ID).strip().zfill(4)]\n",
    "elif BUILD_ALL_BUCKETS:\n",
    "    target_bucket_ids = BUCKET_IDS\n",
    "else:\n",
    "    target_bucket_ids = []\n",
    "\n",
    "print(\"Target buckets:\", target_bucket_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not target_bucket_ids:\n",
    "    print(\"No buckets selected. Set TARGET_BUCKET_ID or TARGET_BUCKET_IDS (or BUILD_ALL_BUCKETS=True).\")\n",
    "else:\n",
    "    for bid in target_bucket_ids:\n",
    "        build_requests_for_bucket(\n",
    "            bucket_id=bid,\n",
    "            allowed_ids=set(ALL_ARXIV_IDS),\n",
    "            skip_ids=SKIP_ARXIV_IDS,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e69961",
   "metadata": {},
   "source": [
    "## 5) Execute bucket (resumable) + usage logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Execute bucket (resumable) + usage logging\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_existing_results(path: Path) -> dict[str, dict]:\n",
    "    done = {}\n",
    "    if not path.exists():\n",
    "        return done\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            cid = rec.get(\"custom_id\")\n",
    "            if cid:\n",
    "                done[cid] = rec\n",
    "    return done\n",
    "\n",
    "def run_resumable(\n",
    "    requests_path: Path,\n",
    "    output_path: Path,\n",
    "    usage_path: Path,\n",
    "    max_workers: int = 30,\n",
    "    max_retries: int = 2,\n",
    "    retry_delay: float = 3.0,\n",
    "):\n",
    "    if client is None:\n",
    "        print(\"[WARN] OpenAI client not initialized. Skipping LLM execution.\")\n",
    "        print(\"       Set ASTRO_OPENAI_ENDPOINT and ASTRO_OPENAI_API_KEY to enable this step.\")\n",
    "        return\n",
    "    if not requests_path.exists():\n",
    "        raise FileNotFoundError(f\"Requests file not found: {requests_path}\")\n",
    "\n",
    "    all_reqs = [json.loads(line) for line in requests_path.open() if line.strip()]\n",
    "    print(f\"[Run] Loaded {len(all_reqs)} requests from {requests_path}\")\n",
    "\n",
    "    existing = load_existing_results(output_path)\n",
    "    done_cids = set()\n",
    "    for cid, rec in existing.items():\n",
    "        if rec.get(\"error\"):\n",
    "            continue\n",
    "        if rec.get(\"parsed\"):\n",
    "            done_cids.add(cid)\n",
    "\n",
    "    pending = [r for r in all_reqs if r[\"custom_id\"] not in done_cids]\n",
    "    print(f\"[Run] {len(pending)} requests pending ({len(done_cids)} already done)\")\n",
    "\n",
    "    if not pending:\n",
    "        print(\"[Run] Nothing to do, all requests already processed.\")\n",
    "        return\n",
    "\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "\n",
    "    def call_one(req: dict) -> dict:\n",
    "        meta = req.get(\"metadata\") or {}\n",
    "        arxiv_id = meta.get(\"arxiv_id\")\n",
    "        prompt_version = meta.get(\"prompt_version\", PROMPT_VERSION)\n",
    "        schema_version = meta.get(\"schema_version\", SCHEMA_VERSION)\n",
    "\n",
    "        last_err = None\n",
    "        for attempt in range(max_retries + 1):\n",
    "            try:\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=MODEL_DEPLOYMENT,\n",
    "                    messages=req[\"body\"][\"messages\"],\n",
    "                    response_format=req[\"body\"].get(\"response_format\"),\n",
    "                )\n",
    "                content = resp.choices[0].message.content\n",
    "                try:\n",
    "                    parsed = json.loads(content)\n",
    "                    parse_error = None\n",
    "                except Exception as e:\n",
    "                    parsed = None\n",
    "                    parse_error = str(e)\n",
    "\n",
    "                schema_errors = validate_record(parsed) if parsed else []\n",
    "\n",
    "                usage = getattr(resp, \"usage\", None)\n",
    "                usage_rec = None\n",
    "                if usage is not None:\n",
    "                    usage_rec = {\n",
    "                        \"custom_id\": req.get(\"custom_id\"),\n",
    "                        \"arxiv_id\": arxiv_id,\n",
    "                        \"prompt_tokens\": usage.prompt_tokens,\n",
    "                        \"completion_tokens\": usage.completion_tokens,\n",
    "                        \"total_tokens\": usage.total_tokens,\n",
    "                    }\n",
    "\n",
    "                return {\n",
    "                    \"custom_id\": req.get(\"custom_id\"),\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"prompt_version\": prompt_version,\n",
    "                    \"schema_version\": schema_version,\n",
    "                    \"model\": MODEL,\n",
    "                    \"raw_response\": content,\n",
    "                    \"parsed\": parsed,\n",
    "                    \"error\": None,\n",
    "                    \"parse_error\": parse_error,\n",
    "                    \"schema_errors\": schema_errors,\n",
    "                    \"usage\": usage_rec,\n",
    "                    \"metadata\": meta,\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "                if \"rate limit\" in last_err.lower() and attempt < max_retries:\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"custom_id\": req.get(\"custom_id\"),\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"prompt_version\": prompt_version,\n",
    "            \"schema_version\": schema_version,\n",
    "            \"model\": MODEL,\n",
    "            \"raw_response\": None,\n",
    "            \"parsed\": None,\n",
    "            \"error\": last_err or \"unknown error\",\n",
    "            \"parse_error\": None,\n",
    "            \"schema_errors\": [],\n",
    "            \"usage\": None,\n",
    "            \"metadata\": meta,\n",
    "        }\n",
    "\n",
    "    with output_path.open(\"a\") as fout, usage_path.open(\"a\") as uout, ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(call_one, r) for r in pending]\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"LLM run {requests_path.name}\"):\n",
    "            res = fut.result()\n",
    "            fout.write(json.dumps(res) + \"\\n\")\n",
    "            if res.get(\"usage\"):\n",
    "                uout.write(json.dumps(res[\"usage\"]) + \"\\n\")\n",
    "                total_prompt_tokens += res[\"usage\"].get(\"prompt_tokens\", 0) or 0\n",
    "                total_completion_tokens += res[\"usage\"].get(\"completion_tokens\", 0) or 0\n",
    "            else:\n",
    "                # helpful debugging\n",
    "                if res.get(\"error\") or res.get(\"parse_error\"):\n",
    "                    print(\"Failure:\", res.get(\"custom_id\"), res.get(\"error\"), res.get(\"parse_error\"))\n",
    "\n",
    "\n",
    "    print(\"[Run] Done. Wrote/updated outputs in\", output_path)\n",
    "    print(f\"  Chunk prompt tokens    : {total_prompt_tokens}\")\n",
    "    print(f\"  Chunk completion tokens: {total_completion_tokens}\")\n",
    "\n",
    "RUN_BUCKET_IDS = target_bucket_ids\n",
    "for bid in RUN_BUCKET_IDS:\n",
    "    bucket_entry = bucket_index.get(bid)\n",
    "    if bucket_entry is None:\n",
    "        print(f\"Bucket {bid} not found, skipping\")\n",
    "        continue\n",
    "    req_path = REQ_DIR / f\"bucket_{bid}_requests.jsonl\"\n",
    "    out_path = OUT_DIR / f\"bucket_{bid}_outputs.jsonl\"\n",
    "    usage_path = USAGE_DIR / f\"bucket_{bid}_usage.jsonl\"\n",
    "    run_resumable(req_path, out_path, usage_path, max_workers=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d673734",
   "metadata": {},
   "source": [
    "## 6) Post-process: parse outputs -> edges CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 6 (post-extraction consolidation -> validated mentions)\n",
    "#\n",
    "# INPUTS (expected to already exist):\n",
    "#   - OUT_DIR / bucket_*_outputs.jsonl\n",
    "#       Each line is a record with keys like:\n",
    "#         custom_id, arxiv_id, parsed (or raw_response), error, parse_error, schema_errors, ...\n",
    "#   - validate_record(parsed_dict) -> list[str]\n",
    "#       schema validator (already defined earlier in the notebook)\n",
    "#\n",
    "# OUTPUTS (written by this single cell):\n",
    "#   - RUN_DIR / outputs_latest.jsonl\n",
    "#       One \"latest\" record per custom_id (paper), with parsed recovered when possible.\n",
    "#   - RUN_DIR / outputs_latest_summary.csv\n",
    "#       Paper-level summary: ok/error counts + schema error counts (fatal vs ignored).\n",
    "#   - RUN_DIR / paper_object_edges_llm_mentions.jsonl\n",
    "#       Mention-level edges after PER-OBJECT validation/auto-fixes:\n",
    "#         (arxiv_id, object_name_norm, role, study_mode, evidence_span, ...)\n",
    "#   - RUN_DIR / paper_object_edges_llm_mentions.jsonl\n",
    "#       Dropped-mention audit log (why each object mention was dropped).\n",
    "#   - RUN_DIR / unique_object_names_llm.json\n",
    "#       Unique normalized object names from *valid mentions* (used for SIMBAD resolution).\n",
    "#   - RUN_DIR / paper_object_mention_counts_llm.csv\n",
    "#       Per-paper count of valid mentions emitted.\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Discover bucket outputs\n",
    "# -----------------------------\n",
    "OUTPUT_GLOB = \"bucket_*_outputs.jsonl\"\n",
    "out_paths = sorted(OUT_DIR.glob(OUTPUT_GLOB))\n",
    "assert out_paths, f\"No outputs found in {OUT_DIR} matching {OUTPUT_GLOB}. Did the extraction finish?\"\n",
    "print(\"Buckets found:\", len(out_paths))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Record-level helpers\n",
    "# -----------------------------\n",
    "# Treat evidence span length as warning (not a failure).\n",
    "IGNORABLE_SCHEMA_ERROR_SUFFIXES = (\n",
    "    \"evidence_span too long\",  # common exact phrasing\n",
    "    \"evidence_span\",           # broad suffix fallback (your choice)\n",
    ")\n",
    "\n",
    "def filter_schema_errors(errs: list[str] | None) -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Returns (fatal_errors, ignored_errors) at RECORD level.\"\"\"\n",
    "    errs = errs or []\n",
    "    ignored, fatal = [], []\n",
    "    for e in errs:\n",
    "        if any(str(e).endswith(sfx) for sfx in IGNORABLE_SCHEMA_ERROR_SUFFIXES):\n",
    "            ignored.append(e)\n",
    "        else:\n",
    "            fatal.append(e)\n",
    "    return fatal, ignored\n",
    "\n",
    "def is_ok_record(rec: dict) -> bool:\n",
    "    \"\"\"OK record means: no API error, parsed present, and no fatal schema errors.\"\"\"\n",
    "    if rec.get(\"error\") is not None:\n",
    "        return False\n",
    "    if rec.get(\"parsed\") is None:\n",
    "        return False\n",
    "    fatal, _ignored = filter_schema_errors(rec.get(\"schema_errors\") or [])\n",
    "    return len(fatal) == 0\n",
    "\n",
    "def iter_jsonl(path: Path):\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "_JSON_OBJ_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
    "\n",
    "def try_salvage_parsed(raw_text: str | None):\n",
    "    \"\"\"Recover JSON object if the model wrapped JSON with extra text.\"\"\"\n",
    "    if not isinstance(raw_text, str) or not raw_text.strip():\n",
    "        return None\n",
    "    m = _JSON_OBJ_RE.search(raw_text)\n",
    "    if not m:\n",
    "        return None\n",
    "    cand = m.group(0).strip()\n",
    "    try:\n",
    "        return json.loads(cand)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def slim_record(rec: dict) -> dict:\n",
    "    \"\"\"Keep only what we need downstream (avoid duplicating huge raw responses).\"\"\"\n",
    "    return {\n",
    "        \"custom_id\": rec.get(\"custom_id\"),\n",
    "        \"arxiv_id\": rec.get(\"arxiv_id\"),\n",
    "        \"prompt_version\": rec.get(\"prompt_version\"),\n",
    "        \"schema_version\": rec.get(\"schema_version\"),\n",
    "        \"model\": rec.get(\"model\"),\n",
    "        \"error\": rec.get(\"error\"),\n",
    "        \"parse_error\": rec.get(\"parse_error\"),\n",
    "        \"schema_errors\": rec.get(\"schema_errors\") or [],\n",
    "        \"metadata\": rec.get(\"metadata\") or {},\n",
    "        \"parsed\": rec.get(\"parsed\"),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load buckets -> latest_by_cid (with salvage)\n",
    "# -----------------------------\n",
    "latest_by_cid: dict[str, dict] = {}\n",
    "bucket_line_counts: dict[str, int] = {}\n",
    "bucket_salvaged_counts: dict[str, int] = {}\n",
    "\n",
    "for p in out_paths:\n",
    "    salvaged_here = 0\n",
    "    n_lines = 0\n",
    "    for rec in iter_jsonl(p):\n",
    "        n_lines += 1\n",
    "        cid = rec.get(\"custom_id\")\n",
    "        if not cid:\n",
    "            continue\n",
    "\n",
    "        # Salvage if parse failed but raw_response exists and error==None\n",
    "        if (rec.get(\"parsed\") is None) and (rec.get(\"error\") is None) and rec.get(\"raw_response\"):\n",
    "            salvaged = try_salvage_parsed(rec.get(\"raw_response\"))\n",
    "            if salvaged is not None:\n",
    "                errs = validate_record(salvaged)  # list[str]\n",
    "                fatal, ignored = filter_schema_errors(errs)\n",
    "                if not fatal:\n",
    "                    rec[\"parsed\"] = salvaged\n",
    "                    rec[\"parse_error\"] = None\n",
    "                    rec[\"schema_errors\"] = errs\n",
    "                    rec[\"ignored_schema_errors\"] = ignored\n",
    "                    salvaged_here += 1\n",
    "\n",
    "        latest_by_cid[cid] = slim_record(rec)\n",
    "\n",
    "    bucket_line_counts[p.name] = n_lines\n",
    "    bucket_salvaged_counts[p.name] = salvaged_here\n",
    "\n",
    "print(\"Unique custom_ids:\", len(latest_by_cid))\n",
    "print(\"Total parse salvaged:\", sum(bucket_salvaged_counts.values()))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Paper-level summary + sanity checks\n",
    "# -----------------------------\n",
    "summary_rows = []\n",
    "for cid, rec in latest_by_cid.items():\n",
    "    parsed = rec.get(\"parsed\")\n",
    "    n_obj = None\n",
    "    if isinstance(parsed, dict) and isinstance(parsed.get(\"objects\"), list):\n",
    "        n_obj = len(parsed[\"objects\"])\n",
    "\n",
    "    ok = is_ok_record(rec)\n",
    "    fatal, ignored = filter_schema_errors(rec.get(\"schema_errors\") or [])\n",
    "\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"custom_id\": cid,\n",
    "            \"arxiv_id\": rec.get(\"arxiv_id\"),\n",
    "            \"ok\": ok,\n",
    "            \"n_objects\": n_obj,\n",
    "            \"error\": rec.get(\"error\"),\n",
    "            \"parse_error\": rec.get(\"parse_error\"),\n",
    "            \"n_schema_errors_total\": len(rec.get(\"schema_errors\") or []),\n",
    "            \"n_schema_errors_fatal\": len(fatal),\n",
    "            \"n_schema_errors_ignored\": len(ignored),\n",
    "            \"prompt_version\": rec.get(\"prompt_version\"),\n",
    "            \"schema_version\": rec.get(\"schema_version\"),\n",
    "            \"model\": rec.get(\"model\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "runs_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "total = len(runs_df)\n",
    "ok_cnt = int(runs_df[\"ok\"].sum())\n",
    "print(f\"Total unique papers (custom_ids): {total:,}\")\n",
    "print(f\"OK papers (parsed + record-schema-ok): {ok_cnt:,}  ({ok_cnt/total:.1%})\")\n",
    "\n",
    "# Duplicate arxiv_id among OK papers\n",
    "ok_df = runs_df[runs_df[\"ok\"]].copy()\n",
    "dups = ok_df[\"arxiv_id\"][ok_df[\"arxiv_id\"].duplicated()].unique().tolist()\n",
    "print(\"Duplicate arxiv_id among OK papers:\", len(dups))\n",
    "assert len(dups) == 0, f\"Unexpected duplicates among OK arxiv_ids (showing up to 5): {dups[:5]}\"\n",
    "\n",
    "# Error summaries\n",
    "err_types = Counter((runs_df[\"error\"].fillna(\"\")).tolist())\n",
    "parse_err_types = Counter((runs_df[\"parse_error\"].fillna(\"\")).tolist())\n",
    "print(\"\\nTop `error` values (incl empty = ok-ish):\")\n",
    "for k, v in err_types.most_common(8):\n",
    "    label = k if k else \"<none>\"\n",
    "    print(f\"  {label[:80]:80s}  {v:,}\")\n",
    "\n",
    "print(\"\\nTop `parse_error` values (incl empty):\")\n",
    "for k, v in parse_err_types.most_common(8):\n",
    "    label = k if k else \"<none>\"\n",
    "    print(f\"  {label[:80]:80s}  {v:,}\")\n",
    "\n",
    "bad_schema = runs_df[runs_df[\"n_schema_errors_fatal\"] > 0]\n",
    "warn_schema = runs_df[runs_df[\"n_schema_errors_ignored\"] > 0]\n",
    "print(\"\\nRecords with FATAL schema errors:\", len(bad_schema))\n",
    "print(\"Records with any IGNORED schema errors:\", len(warn_schema))\n",
    "\n",
    "# Spot-check parsed arxiv_id matches envelope\n",
    "spot = ok_df.sample(min(25, len(ok_df)), random_state=1)[\"custom_id\"].tolist()\n",
    "mismatches = []\n",
    "for cid in spot:\n",
    "    rec = latest_by_cid[cid]\n",
    "    parsed = rec.get(\"parsed\") or {}\n",
    "    if parsed.get(\"arxiv_id\") != rec.get(\"arxiv_id\"):\n",
    "        mismatches.append((cid, rec.get(\"arxiv_id\"), parsed.get(\"arxiv_id\")))\n",
    "print(\"Spot-check mismatched arxiv_id:\", len(mismatches))\n",
    "if mismatches:\n",
    "    print(mismatches[:5])\n",
    "    raise AssertionError(\"Found mismatched arxiv_id between envelope and parsed payload.\")\n",
    "\n",
    "# Persist latest records + summary\n",
    "CONSOLIDATED_LATEST_PATH = RUN_DIR / \"outputs_latest.jsonl\"\n",
    "with CONSOLIDATED_LATEST_PATH.open(\"w\") as f:\n",
    "    for cid, rec in latest_by_cid.items():\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "print(\"Wrote:\", CONSOLIDATED_LATEST_PATH)\n",
    "\n",
    "RUN_SUMMARY_PATH = RUN_DIR / \"outputs_latest_summary.csv\"\n",
    "runs_df.to_csv(RUN_SUMMARY_PATH, index=False)\n",
    "print(\"Wrote:\", RUN_SUMMARY_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Mention-level flattening with PER-OBJECT validation\n",
    "# -----------------------------\n",
    "def norm_object_name(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = s.strip()\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "# Parse object-indexed schema errors from validator messages\n",
    "_OBJ_ERR_RE = re.compile(r\"^objects\\[(\\d+)\\]\\.(.+)$\")\n",
    "\n",
    "def per_object_errors(schema_errors: list[str]) -> dict[int, list[str]]:\n",
    "    out: dict[int, list[str]] = defaultdict(list)\n",
    "    for e in schema_errors or []:\n",
    "        m = _OBJ_ERR_RE.match(str(e))\n",
    "        if not m:\n",
    "            continue\n",
    "        i = int(m.group(1))\n",
    "        msg = m.group(2)\n",
    "        out[i].append(msg)\n",
    "    return out\n",
    "\n",
    "# Object-level ignore policy: same spirit as record-level\n",
    "IGNORABLE_OBJ_ERROR_SUFFIXES = (\n",
    "    \"evidence_span too long\",\n",
    "    \"evidence_span\",\n",
    ")\n",
    "\n",
    "def split_obj_errors(errs: list[str] | None) -> tuple[list[str], list[str]]:\n",
    "    errs = errs or []\n",
    "    ignored, fatal = [], []\n",
    "    for e in errs:\n",
    "        if any(str(e).endswith(sfx) for sfx in IGNORABLE_OBJ_ERROR_SUFFIXES):\n",
    "            ignored.append(e)\n",
    "        else:\n",
    "            fatal.append(e)\n",
    "    return fatal, ignored\n",
    "\n",
    "# Auto-fix common inconsistency patterns\n",
    "NAME_MULTI_DESIG_TOKEN = \"name violates single-designation rule\"\n",
    "MODE_INCONSIST_TOKEN = \"study_mode not_applicable inconsistent with role=\"\n",
    "\n",
    "def auto_fix_object(obj: dict, obj_errs: list[str]) -> tuple[dict, list[str]]:\n",
    "    fix_notes = []\n",
    "    new_obj = dict(obj)\n",
    "\n",
    "    role = new_obj.get(\"role\")\n",
    "    mode = new_obj.get(\"study_mode\")\n",
    "\n",
    "    # If role implies substantive but mode says not_applicable -> unknown\n",
    "    if any(MODE_INCONSIST_TOKEN in e for e in (obj_errs or [])) and mode == \"not_applicable\":\n",
    "        new_obj[\"study_mode\"] = \"unknown\"\n",
    "        fix_notes.append(\"coerced study_mode not_applicable -> unknown (role implies substantive)\")\n",
    "\n",
    "    # If comparison/reference but mode is active -> not_applicable\n",
    "    if role == \"comparison_or_reference\" and mode in {\"new_observation\", \"archival_or_reanalysis\", \"catalog_compilation\", \"simulation_or_theory\"}:\n",
    "        new_obj[\"study_mode\"] = \"not_applicable\"\n",
    "        fix_notes.append(\"coerced study_mode -> not_applicable for comparison_or_reference\")\n",
    "\n",
    "    return new_obj, fix_notes\n",
    "\n",
    "MENTIONS_JSONL = RUN_DIR / \"paper_object_edges_llm_mentions.jsonl\"\n",
    "MENTIONS_AUDIT_CSV = RUN_DIR / \"paper_object_edges_llm_mentions_audit.csv\"\n",
    "\n",
    "if MENTIONS_JSONL.exists():\n",
    "    MENTIONS_JSONL.unlink()\n",
    "if MENTIONS_AUDIT_CSV.exists():\n",
    "    MENTIONS_AUDIT_CSV.unlink()\n",
    "\n",
    "n_mentions = 0\n",
    "n_valid_mentions = 0\n",
    "n_papers_seen = 0\n",
    "n_papers_emitted = 0\n",
    "\n",
    "unique_names = set()\n",
    "per_paper_counts = defaultdict(int)\n",
    "\n",
    "audit_rows = []\n",
    "audit_counter = Counter()\n",
    "\n",
    "with MENTIONS_JSONL.open(\"w\") as f:\n",
    "    for cid, rec in tqdm(latest_by_cid.items(), total=len(latest_by_cid), desc=\"Flatten mentions (per-object validate)\"):\n",
    "\n",
    "        parsed = rec.get(\"parsed\")\n",
    "        if rec.get(\"error\") is not None or parsed is None:\n",
    "            audit_counter[\"skip_record_error_or_unparsed\"] += 1\n",
    "            continue\n",
    "\n",
    "        arxiv_id = rec.get(\"arxiv_id\")\n",
    "        if not arxiv_id:\n",
    "            audit_counter[\"skip_record_missing_arxiv_id\"] += 1\n",
    "            continue\n",
    "\n",
    "        objs = parsed.get(\"objects\") or []\n",
    "        if not isinstance(objs, list):\n",
    "            audit_counter[\"skip_record_objects_not_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        n_papers_seen += 1\n",
    "\n",
    "        schema_errors = rec.get(\"schema_errors\") or []\n",
    "        obj_err_map = per_object_errors(schema_errors)\n",
    "\n",
    "        emitted_any = False\n",
    "\n",
    "        for i, obj in enumerate(objs):\n",
    "            n_mentions += 1\n",
    "            if not isinstance(obj, dict):\n",
    "                audit_counter[\"skip_obj_not_dict\"] += 1\n",
    "                continue\n",
    "\n",
    "            obj_errs = obj_err_map.get(i, [])\n",
    "            obj2, fix_notes = auto_fix_object(obj, obj_errs)\n",
    "\n",
    "            name_norm = norm_object_name(obj2.get(\"name\", \"\"))\n",
    "            if not name_norm:\n",
    "                audit_counter[\"skip_obj_empty_name\"] += 1\n",
    "                continue\n",
    "\n",
    "            fatal0, ignored0 = split_obj_errors(obj_errs)\n",
    "\n",
    "            # Multi-designation name: drop mention (unless you later implement splitting)\n",
    "            if any(NAME_MULTI_DESIG_TOKEN in e for e in fatal0):\n",
    "                audit_counter[\"skip_obj_multi_designation_name\"] += 1\n",
    "                audit_rows.append({\n",
    "                    \"custom_id\": cid,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"obj_index\": i,\n",
    "                    \"object_name_norm\": name_norm,\n",
    "                    \"status\": \"dropped\",\n",
    "                    \"reason\": \"multi_designation_name\",\n",
    "                    \"fatal_errors\": fatal0,\n",
    "                    \"ignored_errors\": ignored0,\n",
    "                    \"fix_notes\": fix_notes,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Any other fatal errors: drop mention\n",
    "            if fatal0:\n",
    "                audit_counter[\"skip_obj_other_fatal_errors\"] += 1\n",
    "                audit_rows.append({\n",
    "                    \"custom_id\": cid,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"obj_index\": i,\n",
    "                    \"object_name_norm\": name_norm,\n",
    "                    \"status\": \"dropped\",\n",
    "                    \"reason\": \"fatal_schema_errors\",\n",
    "                    \"fatal_errors\": fatal0,\n",
    "                    \"ignored_errors\": ignored0,\n",
    "                    \"fix_notes\": fix_notes,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"custom_id\": cid,\n",
    "                \"arxiv_id\": arxiv_id,\n",
    "                \"object_name\": obj2.get(\"name\", \"\"),\n",
    "                \"object_name_norm\": name_norm,\n",
    "                \"role\": obj2.get(\"role\"),\n",
    "                \"study_mode\": obj2.get(\"study_mode\"),\n",
    "                \"evidence_source\": obj2.get(\"evidence_source\"),\n",
    "                \"evidence_span\": obj2.get(\"evidence_span\"),\n",
    "                \"prompt_version\": rec.get(\"prompt_version\"),\n",
    "                \"schema_version\": rec.get(\"schema_version\"),\n",
    "                \"model\": rec.get(\"model\"),\n",
    "                # audit fields\n",
    "                \"obj_index\": i,\n",
    "                \"validator_fatal_errors\": fatal0,\n",
    "                \"validator_ignored_errors\": ignored0,\n",
    "                \"fix_notes\": fix_notes,\n",
    "            }\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            n_valid_mentions += 1\n",
    "            emitted_any = True\n",
    "            unique_names.add(name_norm)\n",
    "            per_paper_counts[arxiv_id] += 1\n",
    "\n",
    "        if emitted_any:\n",
    "            n_papers_emitted += 1\n",
    "        else:\n",
    "            audit_counter[\"papers_with_zero_valid_mentions\"] += 1\n",
    "\n",
    "print(\"\\nMention flatten summary:\")\n",
    "print(\"  Papers seen (parsed + no API error):\", f\"{n_papers_seen:,}\")\n",
    "print(\"  Papers emitted (>=1 valid mention):\", f\"{n_papers_emitted:,}\")\n",
    "print(\"  Total mentions (raw objects):\", f\"{n_mentions:,}\")\n",
    "print(\"  Valid mentions written:\", f\"{n_valid_mentions:,}\")\n",
    "print(\"  Unique object names:\", f\"{len(unique_names):,}\")\n",
    "print(\"  Wrote:\", MENTIONS_JSONL)\n",
    "\n",
    "print(\"\\nTop mention audit counters:\")\n",
    "for k, v in audit_counter.most_common(12):\n",
    "    print(f\"  {k:40s} {v:,}\")\n",
    "\n",
    "# Persist unique names for SIMBAD resolution step\n",
    "UNIQUE_NAMES_PATH = RUN_DIR / \"unique_object_names_llm.json\"\n",
    "with UNIQUE_NAMES_PATH.open(\"w\") as f:\n",
    "    json.dump(sorted(unique_names), f, indent=2, ensure_ascii=False)\n",
    "print(\"Wrote:\", UNIQUE_NAMES_PATH)\n",
    "\n",
    "# Per-paper mention counts\n",
    "counts_df = pd.DataFrame(\n",
    "    [{\"arxiv_id\": k, \"n_object_mentions\": v} for k, v in per_paper_counts.items()]\n",
    ").sort_values(\"n_object_mentions\", ascending=False)\n",
    "\n",
    "COUNTS_PATH = RUN_DIR / \"paper_object_mention_counts_llm.csv\"\n",
    "counts_df.to_csv(COUNTS_PATH, index=False)\n",
    "print(\"Wrote:\", COUNTS_PATH)\n",
    "\n",
    "# Dropped mention audit\n",
    "if audit_rows:\n",
    "    audit_df = pd.DataFrame(audit_rows)\n",
    "    audit_df.to_csv(MENTIONS_AUDIT_CSV, index=False)\n",
    "    print(\"Wrote audit:\", MENTIONS_AUDIT_CSV, \"rows:\", len(audit_df))\n",
    "else:\n",
    "    print(\"No dropped mentions logged (audit empty).\")\n",
    "\n",
    "counts_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Join concepts -> aggregate concept-object edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4accc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 7: Produce concept–object edges (unresolved name space)\n",
    "# (release-safe: paths + weights loaded from config/table1.yaml)\n",
    "#\n",
    "#   concept_object_edges_unresolved_llm.csv.gz\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Load YAML config (paths + weights) ----\n",
    "CONFIG_PATH = Path(os.environ.get(\"CONFIG_PATH\", \"config/table1.yaml\")).resolve()\n",
    "print(f\"[config] Using CONFIG_PATH = {CONFIG_PATH}\")\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config not found: {CONFIG_PATH}\\n\"\n",
    "        \"Double check the path, or set env var CONFIG_PATH=/path/to/table1.yaml\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import yaml  # pip install pyyaml if missing\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency: pyyaml (needed to read config/table1.yaml). \"\n",
    "        \"Install with: pip install pyyaml\"\n",
    "    ) from e\n",
    "\n",
    "with CONFIG_PATH.open() as f:\n",
    "    cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "paths_cfg = cfg.get(\"paths\", {}) or {}\n",
    "weights_cfg = cfg.get(\"weights\", {}) or {}\n",
    "\n",
    "DATA_DIR_CFG = Path(paths_cfg.get(\"data_dir\", \"data\"))\n",
    "OUT_DIR_CFG  = Path(paths_cfg.get(\"out_dir\", str(RUN_DIR if \"RUN_DIR\" in globals() else \"outputs\")))\n",
    "\n",
    "def _resolve_path(base_dir: Path, p: str | None) -> Path | None:\n",
    "    if p is None:\n",
    "        return None\n",
    "    p = str(p).strip()\n",
    "    if not p:\n",
    "        return None\n",
    "    pp = Path(p)\n",
    "    return pp if pp.is_absolute() else (base_dir / pp).resolve()\n",
    "\n",
    "# Key input paths from YAML\n",
    "PAPER_CONCEPT_EDGES_PATH = _resolve_path(DATA_DIR_CFG, paths_cfg.get(\"concept_map\", \"papers_concepts_mapping.csv\"))\n",
    "PO_MENTIONS_PATH = _resolve_path(OUT_DIR_CFG, paths_cfg.get(\"po_mentions\", \"paper_object_edges_llm_mentions.jsonl\"))\n",
    "\n",
    "print(f\"[config] PAPER_CONCEPT_EDGES_PATH = {PAPER_CONCEPT_EDGES_PATH} | exists = {PAPER_CONCEPT_EDGES_PATH.exists() if PAPER_CONCEPT_EDGES_PATH else False}\")\n",
    "print(f\"[config] PO_MENTIONS_PATH         = {PO_MENTIONS_PATH} | exists = {PO_MENTIONS_PATH.exists() if PO_MENTIONS_PATH else False}\")\n",
    "\n",
    "if PAPER_CONCEPT_EDGES_PATH is None or not PAPER_CONCEPT_EDGES_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing paper–concept mapping CSV at: {PAPER_CONCEPT_EDGES_PATH}\\n\"\n",
    "        \"Double check cfg.paths.concept_map and cfg.paths.data_dir in table1.yaml.\"\n",
    "    )\n",
    "\n",
    "if PO_MENTIONS_PATH is None or not PO_MENTIONS_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing mentions JSONL at: {PO_MENTIONS_PATH}\\n\"\n",
    "        \"Double check cfg.paths.po_mentions and cfg.paths.out_dir in table1.yaml.\"\n",
    "    )\n",
    "\n",
    "# Weights from YAML\n",
    "ROLE_WEIGHT = (weights_cfg.get(\"role_weight\", {}) or {})\n",
    "STUDY_MODE_MULT = (weights_cfg.get(\"study_mode_mult\", {}) or {})\n",
    "CONTEXT_ROLES = set(weights_cfg.get(\"context_roles\", []) or [])\n",
    "\n",
    "if not ROLE_WEIGHT or not STUDY_MODE_MULT:\n",
    "    raise ValueError(\n",
    "        \"weights.role_weight and/or weights.study_mode_mult missing or empty in table1.yaml. \"\n",
    "        \"Double check your config.\"\n",
    "    )\n",
    "\n",
    "print(f\"[config] Loaded ROLE_WEIGHT keys: {len(ROLE_WEIGHT)}\")\n",
    "print(f\"[config] Loaded STUDY_MODE_MULT keys: {len(STUDY_MODE_MULT)}\")\n",
    "print(f\"[config] Loaded CONTEXT_ROLES: {sorted(CONTEXT_ROLES)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7.1) Load paper–concept edges\n",
    "# -----------------------------\n",
    "paper_concept_df = pd.read_csv(PAPER_CONCEPT_EDGES_PATH, dtype={\"arxiv_id\": \"string\", \"label\": \"int32\"})\n",
    "expected_cols = {\"arxiv_id\", \"label\"}\n",
    "if set(paper_concept_df.columns) != expected_cols:\n",
    "    raise ValueError(f\"Unexpected columns: {paper_concept_df.columns.tolist()} (expected {sorted(expected_cols)})\")\n",
    "\n",
    "paper_concept_df = paper_concept_df.dropna(subset=[\"arxiv_id\", \"label\"]).copy()\n",
    "paper_concept_df[\"arxiv_id\"] = paper_concept_df[\"arxiv_id\"].str.strip()\n",
    "paper_concept_df = paper_concept_df[paper_concept_df[\"arxiv_id\"].str.len() > 0]\n",
    "\n",
    "paper_concept_df[\"weight\"] = np.float32(1.0)\n",
    "\n",
    "before = len(paper_concept_df)\n",
    "paper_concept_df = paper_concept_df.drop_duplicates(subset=[\"arxiv_id\", \"label\"]).reset_index(drop=True)\n",
    "print(\"paper_concept_df:\", paper_concept_df.shape, \"dropped dups:\", before - len(paper_concept_df))\n",
    "paper_concept_df.head()\n",
    "\n",
    "# -----------------------------\n",
    "# 7.1b) Load mentions + coverage checks\n",
    "# -----------------------------\n",
    "po_mentions = pd.read_json(PO_MENTIONS_PATH, lines=True, dtype={\"arxiv_id\": \"string\"})\n",
    "print(\"po_mentions:\", po_mentions.shape)\n",
    "\n",
    "pc_papers = set(paper_concept_df[\"arxiv_id\"].unique().tolist())\n",
    "po_papers = set(po_mentions[\"arxiv_id\"].unique().tolist())\n",
    "\n",
    "print(\"Unique papers in paper_concept_df:\", len(pc_papers))\n",
    "print(\"Unique papers in po_mentions:\", len(po_papers))\n",
    "print(\"Overlap papers:\", len(pc_papers & po_papers))\n",
    "print(\"Papers with objects but no concepts:\", len(po_papers - pc_papers))\n",
    "print(\"Papers with concepts but no objects:\", len(pc_papers - po_papers))\n",
    "\n",
    "# -----------------------------\n",
    "# 7.2) Aggregate paper–object edges (legacy tier3 semantics)\n",
    "# -----------------------------\n",
    "po_mentions = po_mentions.copy()\n",
    "po_mentions[\"role_weight\"] = po_mentions[\"role\"].map(ROLE_WEIGHT).fillna(np.float32(0.75)).astype(\"float32\")\n",
    "po_mentions[\"mode_mult\"] = po_mentions[\"study_mode\"].map(STUDY_MODE_MULT).fillna(np.float32(0.50)).astype(\"float32\")\n",
    "po_mentions[\"mention_weight\"] = (po_mentions[\"role_weight\"] * po_mentions[\"mode_mult\"]).astype(\"float32\")\n",
    "\n",
    "# Drop context-only mentions via weight==0 (typically study_mode=not_applicable)\n",
    "po_mentions_f = po_mentions[po_mentions[\"mention_weight\"] > 0].copy()\n",
    "print(\"Mentions kept (mention_weight>0):\", len(po_mentions_f), \"/\", len(po_mentions))\n",
    "\n",
    "def _uniq_sorted(xs):\n",
    "    xs = [x for x in xs if isinstance(x, str) and x]\n",
    "    return sorted(set(xs))\n",
    "\n",
    "def _pick_examples(xs, k=3):\n",
    "    xs = [x for x in xs if isinstance(x, str) and x.strip()]\n",
    "    xs = sorted(xs, key=len)[:k]\n",
    "    return xs\n",
    "\n",
    "po_agg = (\n",
    "    po_mentions_f\n",
    "    .groupby([\"arxiv_id\", \"object_name_norm\"], as_index=False)\n",
    "    .agg(\n",
    "        mention_count=(\"object_name_norm\", \"size\"),\n",
    "        obj_weight=(\"mention_weight\", \"sum\"),\n",
    "        roles=(\"role\", _uniq_sorted),\n",
    "        study_modes=(\"study_mode\", _uniq_sorted),\n",
    "        evidence_sources=(\"evidence_source\", _uniq_sorted),\n",
    "        example_evidence=(\"evidence_span\", _pick_examples),\n",
    "    )\n",
    ")\n",
    "po_agg[\"obj_weight\"] = po_agg[\"obj_weight\"].astype(\"float32\")\n",
    "\n",
    "print(\"po_agg:\", po_agg.shape)\n",
    "\n",
    "PO_AGG_PATH = _resolve_path(OUT_DIR_CFG, paths_cfg.get(\"po_agg\", \"paper_object_edges_llm_agg.parquet\")) or (RUN_DIR / \"paper_object_edges_llm_agg.parquet\")\n",
    "po_agg.to_parquet(PO_AGG_PATH, index=False)\n",
    "print(\"Wrote:\", PO_AGG_PATH)\n",
    "\n",
    "# Legacy tier3 filter: exclude rows that are purely context roles\n",
    "po_used = po_agg[po_agg[\"roles\"].apply(lambda rs: not set(rs).issubset(CONTEXT_ROLES))].copy()\n",
    "print(\"paper–object edges kept after context-only exclusion:\", len(po_used), \"/\", len(po_agg))\n",
    "\n",
    "# -----------------------------\n",
    "# 7.3) Produce concept–object edges (UNRESOLVED)\n",
    "# -----------------------------\n",
    "def build_concept_object_edges(paper_concept: pd.DataFrame, paper_object: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    paper_concept: [arxiv_id, label, weight]\n",
    "    paper_object:  [arxiv_id, object_name_norm, obj_weight]\n",
    "    Returns: [label, object_name_norm, n_papers, total_weight]\n",
    "    \"\"\"\n",
    "    merged = paper_concept.merge(\n",
    "        paper_object[[\"arxiv_id\", \"object_name_norm\", \"obj_weight\"]],\n",
    "        on=\"arxiv_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    merged[\"edge_weight\"] = (merged[\"weight\"].astype(\"float32\") * merged[\"obj_weight\"].astype(\"float32\")).astype(\"float32\")\n",
    "\n",
    "    co = (\n",
    "        merged\n",
    "        .groupby([\"label\", \"object_name_norm\"], as_index=False)\n",
    "        .agg(\n",
    "            n_papers=(\"arxiv_id\", \"nunique\"),\n",
    "            total_weight=(\"edge_weight\", \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    co[\"label\"] = co[\"label\"].astype(\"int32\")\n",
    "    co[\"n_papers\"] = co[\"n_papers\"].astype(\"int32\")\n",
    "    co[\"total_weight\"] = co[\"total_weight\"].astype(\"float32\")\n",
    "    return co.sort_values([\"label\", \"total_weight\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "co_unres = build_concept_object_edges(paper_concept_df, po_used)\n",
    "print(\"concept_object_edges_unresolved:\", co_unres.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 7.3b) Sanity checks\n",
    "# -----------------------------\n",
    "assert co_unres[\"total_weight\"].min() > 0, \"non-positive total_weight\"\n",
    "dups = co_unres.duplicated(subset=[\"label\", \"object_name_norm\"]).sum()\n",
    "assert dups == 0, \"duplicate (label, object_name_norm) rows\"\n",
    "print(f\"ok | edges={len(co_unres):,} | unique labels={co_unres['label'].nunique():,} | unique objects={co_unres['object_name_norm'].nunique():,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7.3c) Write unresolved deliverable (renamed from legacy tier3)\n",
    "# -----------------------------\n",
    "OUT_UNRES = RUN_DIR / \"concept_object_edges_unresolved_llm.csv.gz\"\n",
    "co_unres.to_csv(OUT_UNRES, index=False, compression=\"gzip\")\n",
    "\n",
    "print(\"Wrote:\", OUT_UNRES)\n",
    "\n",
    "co_unres.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) SIMBAD resolution for astro_object + final deliverables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 176014 alias→main_id entries from simbad_alias_cache\n",
      "Loaded 518655 name→main_id entries from simbad_name_resolution_cache_llm_objects_with_otype_and_errors_clean.jsonl\n",
      "Loaded no_match failures: query_name: 355519 | alias_norm: 355527\n",
      "Unique object_name_norm (raw): 339495\n",
      "Unique object_name_clean     : 339494\n",
      "object_freq shape: (339494, 2)\n",
      "Total unique CLEAN object names (tier3 unresolved): 339,494\n",
      "Skipped due to known no_match failures: 155958\n",
      "Mapped via existing SIMBAD data: 140,991\n",
      "Unmapped                       : 42,545\n",
      "Coverage (mapped / total clean): 41.530%\n",
      "Unmapped objects with frequencies: (42545, 3)\n",
      "   object_name_clean  total_n_papers\n",
      "0     PN G035.9-01.1              17\n",
      "1          UDS-10246              10\n",
      "2          UDS 90845              10\n",
      "3          UDS 46645              10\n",
      "4            Mrk 945              10\n",
      "5            Mrk0004              10\n",
      "6          UDS 408.0              10\n",
      "7          UDS 37091              10\n",
      "8            Mrk0007              10\n",
      "9            Mrk0008              10\n",
      "10           UDS-101              10\n",
      "11          UDS-8536              10\n",
      "12         UDS-07447              10\n",
      "13          UDS-022b              10\n",
      "14          UDS-022a              10\n",
      "15           UDS-154              10\n",
      "16          UDS-3583              10\n",
      "17         UDS-31649              10\n",
      "18         UDS-31037              10\n",
      "19          UDS-9730              10\n",
      "Attempting SIMBAD queries for: 42545 CLEAN names (top by total_n_papers)\n",
      "First 20 to_resolve: ['PN G035.9-01.1', 'UDS-10246', 'UDS 90845', 'UDS 46645', 'Mrk 945', 'Mrk0004', 'UDS 408.0', 'UDS 37091', 'Mrk0007', 'Mrk0008', 'UDS-101', 'UDS-8536', 'UDS-07447', 'UDS-022b', 'UDS-022a', 'UDS-154', 'UDS-3583', 'UDS-31649', 'UDS-31037', 'UDS-9730']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving via SIMBAD: 100%|██████████| 42545/42545 [2:29:29<00:00,  4.74it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMBAD query summary:\n",
      "  ok      : 17855\n",
      "  no_match: 24657\n",
      "  error   : 1\n",
      "  rows appended to NAME_CACHE_PATH: 42513\n",
      "  new failures appended: 24657\n",
      "Post-query coverage (clean): 158874/339494 = 46.797%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Section 8 (fixed): Resolve concept-object \"unresolved\" names to SIMBAD main_id\n",
    "# using existing caches + negative-cache failures, with ONE-TIME cleaning.\n",
    "#\n",
    "# INPUTS:\n",
    "#   - RUN_DIR / concept_object_edges_tier3_unresolved_llm.csv.gz\n",
    "#       columns: [label, object_name_norm, n_papers, total_weight]\n",
    "#   - cache dir files:\n",
    "#       - simbad_alias_cache.jsonl\n",
    "#       - simbad_name_resolution_cache_llm_objects_with_otype_and_errors_clean.jsonl\n",
    "#       - simbad_resolution_failures.jsonl\n",
    "#\n",
    "# OUTPUTS (appended):\n",
    "#   - NAME_CACHE_PATH (append new SIMBAD results)\n",
    "#   - FAILURES_PATH   (append new no_match failures only)\n",
    "#\n",
    "# KEY FIXES:\n",
    "#   - Create object_name_clean ONCE and aggregate on it (prevents hidden-char duplicates)\n",
    "#   - Use clean keys consistently for cache lookups and failure short-circuit\n",
    "#   - Do NOT skip SIMBAD query just because key exists in name2main; only skip if mapped (non-empty main_id)\n",
    "# ============================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from astroquery.simbad import Simbad\n",
    "from astroquery.exceptions import NoResultsWarning\n",
    "\n",
    "\n",
    "\n",
    "# Silence spammy \"No results\" warnings; we handle no-match explicitly\n",
    "warnings.simplefilter(\"ignore\", NoResultsWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (cache dir)\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "# Config-driven paths (release-safe)\n",
    "# -----------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_PATH = Path(os.environ.get(\"CONFIG_PATH\", \"config/table1.yaml\")).resolve()\n",
    "print(f\"[config] Using CONFIG_PATH = {CONFIG_PATH}\")\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config not found: {CONFIG_PATH}\\n\"\n",
    "        \"Double check the path, or set env var CONFIG_PATH=/path/to/table1.yaml\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Missing dependency: pyyaml. Install with: pip install pyyaml\") from e\n",
    "\n",
    "with CONFIG_PATH.open() as f:\n",
    "    cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "paths_cfg = cfg.get(\"paths\", {}) or {}\n",
    "\n",
    "DATA_DIR_CFG = Path(paths_cfg.get(\"data_dir\", \"data\"))\n",
    "OUT_DIR_CFG  = Path(paths_cfg.get(\"out_dir\", str(RUN_DIR if \"RUN_DIR\" in globals() else \"outputs\")))\n",
    "\n",
    "def _resolve_path(base_dir: Path, p):\n",
    "    if p is None:\n",
    "        return None\n",
    "    p = str(p).strip()\n",
    "    if not p:\n",
    "        return None\n",
    "    pp = Path(p)\n",
    "    return pp if pp.is_absolute() else (base_dir / pp)\n",
    "\n",
    "# SIMBAD caches (from YAML)\n",
    "NAME_CACHE_PATH  = _resolve_path(DATA_DIR_CFG, paths_cfg.get(\"simbad_name_cache\"))\n",
    "ALIAS_CACHE_PATH = _resolve_path(DATA_DIR_CFG, paths_cfg.get(\"simbad_alias_cache\"))  # may be null\n",
    "if NAME_CACHE_PATH is None:\n",
    "    raise ValueError(\"paths.simbad_name_cache is missing/null in table1.yaml\")\n",
    "\n",
    "CACHE_DIR = NAME_CACHE_PATH.parent\n",
    "FAILURES_PATH = CACHE_DIR / \"simbad_resolution_failures.jsonl\"\n",
    "\n",
    "print(f\"[paths] DATA_DIR_CFG       = {DATA_DIR_CFG.resolve()}\")\n",
    "print(f\"[paths] OUT_DIR_CFG        = {OUT_DIR_CFG.resolve()}\")\n",
    "print(f\"[paths] NAME_CACHE_PATH    = {NAME_CACHE_PATH.resolve()} | exists = {NAME_CACHE_PATH.exists()}\")\n",
    "print(f\"[paths] ALIAS_CACHE_PATH   = {ALIAS_CACHE_PATH.resolve() if ALIAS_CACHE_PATH else None} | exists = {ALIAS_CACHE_PATH.exists() if ALIAS_CACHE_PATH else False}\")\n",
    "print(f\"[paths] FAILURES_PATH      = {FAILURES_PATH.resolve()} | exists = {FAILURES_PATH.exists()}\")\n",
    "\n",
    "# Ensure cache dir exists\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if not FAILURES_PATH.exists():\n",
    "    FAILURES_PATH.write_text(\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load unresolved concept-object edges (no tier naming)\n",
    "# -----------------------------\n",
    "# Section 7 now writes: RUN_DIR / concept_object_edges_unresolved_llm.csv.gz\n",
    "RUN_DIR = Path(os.environ.get(\"ASTRO_OUT_DIR\", str(OUT_DIR_CFG))) / \"full_extraction_\"\n",
    "co_path = RUN_DIR / \"concept_object_edges_unresolved_llm.csv.gz\"\n",
    "\n",
    "print(f\"[paths] co_path            = {co_path.resolve()} | exists = {co_path.exists()}\")\n",
    "\n",
    "if not co_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing unresolved concept-object edges: {co_path}\\n\"\n",
    "        \"Double check where Section 7 wrote the file, and update the path here accordingly.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def read_jsonl(path: Path):\n",
    "    if not path.exists():\n",
    "        return\n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "def alias_norm(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    return \" \".join(s.split()).lower()\n",
    "\n",
    "# Very safe cleaning (ONLY strip control chars + normalize + whitespace)\n",
    "_CTRL_RE = re.compile(r\"[\\u0000-\\u001F\\u007F-\\u009F]\")\n",
    "\n",
    "def safe_clean_name(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = _CTRL_RE.sub(\"\", s)\n",
    "    s = \" \".join(s.strip().split())\n",
    "    return s\n",
    "\n",
    "def key_norm(s: str) -> str:\n",
    "    # key used for ALL dict/set lookups\n",
    "    return alias_norm(safe_clean_name(s))\n",
    "\n",
    "def load_no_match_failures(path: Path) -> tuple[set[str], set[str]]:\n",
    "    \"\"\"\n",
    "    Only load rows where status == \"no_match\".\n",
    "    Skip iff our query string exactly matches either:\n",
    "      - query_name (after safe_clean_name)\n",
    "      - alias_norm (after key_norm)\n",
    "    \"\"\"\n",
    "    no_match_query_names: set[str] = set()\n",
    "    no_match_alias_norms: set[str] = set()\n",
    "    if not path.exists():\n",
    "        return no_match_query_names, no_match_alias_norms\n",
    "\n",
    "    for row in read_jsonl(path):\n",
    "        if row.get(\"status\") != \"no_match\":\n",
    "            continue\n",
    "        qn = row.get(\"query_name\")\n",
    "        an = row.get(\"alias_norm\")\n",
    "        if isinstance(qn, str) and qn:\n",
    "            no_match_query_names.add(safe_clean_name(qn))   # exact match against obj_clean\n",
    "        if isinstance(an, str) and an:\n",
    "            no_match_alias_norms.add(key_norm(an))          # exact match against key\n",
    "    return no_match_query_names, no_match_alias_norms\n",
    "\n",
    "def to_text(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, bytes):\n",
    "        try:\n",
    "            return x.decode(\"utf-8\", errors=\"replace\")\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "    return str(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Load caches\n",
    "# -----------------------------\n",
    "alias2main: dict[str, str] = {}\n",
    "if ALIAS_CACHE_PATH and ALIAS_CACHE_PATH.exists():\n",
    "    for row in read_jsonl(ALIAS_CACHE_PATH) or []:\n",
    "        main_id = row.get(\"main_id\")\n",
    "        if not isinstance(main_id, str) or not main_id.strip():\n",
    "            continue\n",
    "        aliases = row.get(\"aliases\") or [main_id]\n",
    "        if not isinstance(aliases, list):\n",
    "            aliases = [main_id]\n",
    "        for a in aliases:\n",
    "            if not isinstance(a, str) or not a.strip():\n",
    "                continue\n",
    "            k = key_norm(a)\n",
    "            if k:\n",
    "                alias2main[k] = main_id\n",
    "    print(\"Loaded\", len(alias2main), \"alias→main_id entries from simbad_alias_cache\")\n",
    "else:\n",
    "    print(\"[INFO] alias cache not provided/found; proceeding without it.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load name→main_id cache\n",
    "# -----------------------------\n",
    "name2main: dict[str, str | None] = {}\n",
    "if NAME_CACHE_PATH.exists():\n",
    "    for row in read_jsonl(NAME_CACHE_PATH) or []:\n",
    "        # Prefer alias_norm field, else fallback to query_name\n",
    "        k = row.get(\"alias_norm\")\n",
    "        if not isinstance(k, str) or not k.strip():\n",
    "            qn = row.get(\"query_name\")\n",
    "            if isinstance(qn, str) and qn.strip():\n",
    "                k = qn\n",
    "            else:\n",
    "                continue\n",
    "        k = key_norm(k)\n",
    "        name2main[k] = row.get(\"main_id\")\n",
    "    print(\"Loaded\", len(name2main), \"name→main_id entries from\", NAME_CACHE_PATH.name)\n",
    "else:\n",
    "    print(\"[WARN] name-resolution cache not found:\", NAME_CACHE_PATH)\n",
    "\n",
    "otype_by_main = {}\n",
    "\n",
    "for row in read_jsonl(NAME_CACHE_PATH) or []:\n",
    "    main_id = row.get(\"main_id\")\n",
    "    otype = row.get(\"otype\")\n",
    "    if isinstance(main_id, str) and main_id.strip():\n",
    "        otype_by_main[main_id] = otype\n",
    "\n",
    "no_match_query_names, no_match_alias_norms = load_no_match_failures(FAILURES_PATH)\n",
    "print(\"Loaded no_match failures:\",\n",
    "      \"query_name:\", len(no_match_query_names),\n",
    "      \"| alias_norm:\", len(no_match_alias_norms))\n",
    "\n",
    "# -----------------------------\n",
    "# Load unresolved tier3 edges\n",
    "# -----------------------------\n",
    "assert co_path.exists(), f\"Missing: {co_path}\"\n",
    "\n",
    "concept_object_edges = pd.read_csv(\n",
    "    co_path,\n",
    "    dtype={\"label\": \"int32\", \"object_name_norm\": \"string\", \"n_papers\": \"int32\", \"total_weight\": \"float32\"},\n",
    "    low_memory=False,\n",
    ")\n",
    "required_cols = {\"label\", \"object_name_norm\", \"n_papers\", \"total_weight\"}\n",
    "assert required_cols.issubset(concept_object_edges.columns), f\"Unexpected columns: {concept_object_edges.columns.tolist()}\"\n",
    "\n",
    "# -----------------------------\n",
    "# CRITICAL FIX: clean ONCE, then aggregate / resolve using clean names\n",
    "# -----------------------------\n",
    "concept_object_edges[\"object_name_clean\"] = (\n",
    "    concept_object_edges[\"object_name_norm\"]\n",
    "    .astype(\"string\")\n",
    "    .fillna(\"\")\n",
    "    .map(lambda s: safe_clean_name(s))\n",
    ")\n",
    "\n",
    "# Drop empties after cleaning\n",
    "concept_object_edges = concept_object_edges[concept_object_edges[\"object_name_clean\"].str.len() > 0].copy()\n",
    "\n",
    "print(\"Unique object_name_norm (raw):\", concept_object_edges[\"object_name_norm\"].nunique())\n",
    "print(\"Unique object_name_clean     :\", concept_object_edges[\"object_name_clean\"].nunique())\n",
    "\n",
    "# Frequency table (CLEAN names)\n",
    "object_freq = (\n",
    "    concept_object_edges\n",
    "    .groupby(\"object_name_clean\", as_index=False)[\"n_papers\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"n_papers\": \"total_n_papers\"})\n",
    "    .sort_values(\"total_n_papers\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"object_freq shape:\", object_freq.shape)\n",
    "print(f\"Total unique CLEAN object names (tier3 unresolved): {len(object_freq):,}\")\n",
    "\n",
    "# Precompute lookup key for speed\n",
    "object_freq[\"object_key\"] = object_freq[\"object_name_clean\"].map(key_norm)\n",
    "\n",
    "# -----------------------------\n",
    "# Coverage using caches + failure short-circuit (CLEAN names)\n",
    "# -----------------------------\n",
    "mapped: dict[str, str] = {}     # clean_name -> main_id\n",
    "unmapped: list[str] = []        # clean_name\n",
    "skipped_due_to_failures = 0\n",
    "\n",
    "for obj_clean, k in zip(object_freq[\"object_name_clean\"].tolist(), object_freq[\"object_key\"].tolist()):\n",
    "    # exact-match skip for known no_match failures\n",
    "    if (obj_clean in no_match_query_names) or (k in no_match_alias_norms):\n",
    "        skipped_due_to_failures += 1\n",
    "        continue\n",
    "\n",
    "    main_id = alias2main.get(k) or name2main.get(k)\n",
    "    if isinstance(main_id, str) and main_id.strip():\n",
    "        mapped[obj_clean] = main_id\n",
    "    else:\n",
    "        unmapped.append(obj_clean)\n",
    "\n",
    "print(\"Skipped due to known no_match failures:\", skipped_due_to_failures)\n",
    "print(f\"Mapped via existing SIMBAD data: {len(mapped):,}\")\n",
    "print(f\"Unmapped                       : {len(unmapped):,}\")\n",
    "print(f\"Coverage (mapped / total clean): {len(mapped) / max(1,len(object_freq)):.3%}\")\n",
    "\n",
    "# Unmapped prioritized by freq (clean)\n",
    "unmapped_set = set(unmapped)\n",
    "unmapped_df = (\n",
    "    object_freq[object_freq[\"object_name_clean\"].isin(unmapped_set)]\n",
    "    .copy()\n",
    "    .sort_values(\"total_n_papers\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(\"Unmapped objects with frequencies:\", unmapped_df.shape)\n",
    "print(unmapped_df[[\"object_name_clean\", \"total_n_papers\"]].head(20))\n",
    "\n",
    "# -----------------------------\n",
    "# Query SIMBAD for remaining unmapped, append to NAME cache + failures\n",
    "# -----------------------------\n",
    "\n",
    "# create _simbad and run query loop\n",
    "_simbad = Simbad()\n",
    "_simbad.TIMEOUT = 30\n",
    "_simbad.add_votable_fields(\"otype\")\n",
    "def query_simbad_with_meta(query_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    status: 'ok' | 'no_match' | 'error'\n",
    "    main_id: str | None\n",
    "    otype: str | None\n",
    "    error: str | None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t = _simbad.query_object(query_name)\n",
    "        if t is None or len(t) == 0:\n",
    "            return {\"status\": \"no_match\", \"main_id\": None, \"otype\": None, \"error\": None}\n",
    "        row = t[0]\n",
    "        raw_main = row[\"MAIN_ID\"] if \"MAIN_ID\" in row.colnames else row[row.colnames[0]]\n",
    "        main_id = \" \".join(to_text(raw_main).split()) if raw_main is not None else None\n",
    "        raw_otype = None\n",
    "        if \"OTYPE\" in row.colnames:\n",
    "            raw_otype = row[\"OTYPE\"]\n",
    "        elif \"OTYPE_S\" in row.colnames:\n",
    "            raw_otype = row[\"OTYPE_S\"]\n",
    "        otype = to_text(raw_otype) if raw_otype is not None else None\n",
    "        if not main_id:\n",
    "            return {\"status\": \"error\", \"main_id\": None, \"otype\": None, \"error\": \"empty_MAIN_ID\"}\n",
    "        return {\"status\": \"ok\", \"main_id\": main_id, \"otype\": otype, \"error\": None}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"main_id\": None, \"otype\": None, \"error\": str(e)[:300]}\n",
    "MAX_TO_RESOLVE = 200_000\n",
    "SLEEP_SECONDS  = 0.05\n",
    "to_resolve = unmapped_df[\"object_name_clean\"].head(MAX_TO_RESOLVE).tolist()\n",
    "print(\"Attempting SIMBAD queries for:\", len(to_resolve), \"CLEAN names (top by total_n_papers)\")\n",
    "ok_cnt = no_match_cnt = err_cnt = 0\n",
    "new_rows = 0\n",
    "new_failure_rows = 0\n",
    "# Optional quick visibility (first few queries)\n",
    "print(\"First 20 to_resolve:\", to_resolve[:20])\n",
    "with NAME_CACHE_PATH.open(\"a\") as fout, FAILURES_PATH.open(\"a\") as ff:\n",
    "    for obj_clean in tqdm(to_resolve, desc=\"Resolving via SIMBAD\"):\n",
    "        if not obj_clean:\n",
    "            continue\n",
    "        k = key_norm(obj_clean)\n",
    "        # Skip if known no_match failures (exact match)\n",
    "        if (obj_clean in no_match_query_names) or (k in no_match_alias_norms):\n",
    "            continue\n",
    "        # Skip only if ALREADY RESOLVED (non-empty main_id). Do NOT skip negative/None placeholders.\n",
    "        existing_main = alias2main.get(k) or name2main.get(k)\n",
    "        if isinstance(existing_main, str) and existing_main.strip():\n",
    "            continue\n",
    "        res = query_simbad_with_meta(obj_clean)\n",
    "        if res[\"status\"] == \"ok\":\n",
    "            ok_cnt += 1\n",
    "        elif res[\"status\"] == \"no_match\":\n",
    "            no_match_cnt += 1\n",
    "        else:\n",
    "            err_cnt += 1\n",
    "        record = {\n",
    "            \"query_name\": obj_clean,\n",
    "            \"alias_norm\": k,\n",
    "            \"status\": res[\"status\"],\n",
    "            \"main_id\": res[\"main_id\"],\n",
    "            \"otype\": res[\"otype\"],\n",
    "            \"error\": res[\"error\"],\n",
    "        }\n",
    "        # Update in-memory cache (may be None; that's OK)\n",
    "        name2main[k] = res[\"main_id\"]\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        new_rows += 1\n",
    "        # Only write failures when it's a true no_match\n",
    "        if res[\"status\"] == \"no_match\":\n",
    "            fail_rec = {\n",
    "                \"query_name\": obj_clean,\n",
    "                \"alias_norm\": k,\n",
    "                \"status\": \"no_match\",\n",
    "                \"main_id\": None,\n",
    "                \"otype\": None,\n",
    "                \"error\": None,\n",
    "            }\n",
    "            ff.write(json.dumps(fail_rec, ensure_ascii=False) + \"\\n\")\n",
    "            no_match_query_names.add(obj_clean)\n",
    "            no_match_alias_norms.add(k)\n",
    "            new_failure_rows += 1\n",
    "        # Light throttle\n",
    "        if SLEEP_SECONDS:\n",
    "            time.sleep(SLEEP_SECONDS)\n",
    "print(\"SIMBAD query summary:\")\n",
    "print(\"  ok      :\", ok_cnt)\n",
    "print(\"  no_match:\", no_match_cnt)\n",
    "print(\"  error   :\", err_cnt)\n",
    "print(\"  rows appended to NAME_CACHE_PATH:\", new_rows)\n",
    "print(\"  new failures appended:\", new_failure_rows)\n",
    "# Optional: recompute coverage over CLEAN names\n",
    "mapped2 = 0\n",
    "for obj_clean, k in zip(object_freq[\"object_name_clean\"].tolist(), object_freq[\"object_key\"].tolist()):\n",
    "    main_id = alias2main.get(k) or name2main.get(k)\n",
    "    if isinstance(main_id, str) and main_id.strip():\n",
    "        mapped2 += 1\n",
    "print(f\"Post-query coverage (clean): {mapped2}/{len(object_freq)} = {mapped2 / max(1,len(object_freq)):.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows total: 6,816,113\n",
      "Rows with resolved object_id: 4,258,978 (62.48%)\n",
      "Resolved aggregated edges: (3731041, 5)\n",
      "Unique objects: 103214\n",
      "Wrote: /Users/jinchuli/projects/astro-llm-tools/data/llm_object_extraction_final/full_extraction_v2/concept_object_edges_tier3_resolved_simbad_only_llm_v2.csv.gz\n",
      "Wrote: /Users/jinchuli/projects/astro-llm-tools/data/llm_object_extraction_final/full_extraction_v2/concept_object_edges_tier3_resolved_simbad_only_llm_v2_noReg.csv.gz | dropped: 6097\n",
      "Wrote: /Users/jinchuli/projects/astro-llm-tools/data/llm_object_extraction_final/full_extraction_v2/simbad_object_catalog_llm_v2.csv.gz | rows: 103214\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Section 8b: Materialize resolved concept–object edges (SIMBAD main_id)\n",
    "#\n",
    "# INPUTS:\n",
    "#   - RUN_DIR / concept_object_edges_unresolved_llm.csv.gz\n",
    "#       columns: [label, object_name_norm, n_papers, total_weight]\n",
    "#   - In-memory caches (post-resolution):\n",
    "#       alias2main (key -> main_id)   [optional]\n",
    "#       name2main  (key -> main_id or None)\n",
    "#   - (Optional) otype_by_main (main_id -> otype) if want noReg filtering\n",
    "#\n",
    "# OUTPUTS:\n",
    "#   - RUN_DIR / concept_object_edges_resolved_simbad_only_llm.csv.gz\n",
    "#   - RUN_DIR / concept_object_edges_resolved_simbad_only_llm_noReg.csv.gz  (optional)\n",
    "#   - RUN_DIR / simbad_object_catalog_llm.csv.gz                             (optional)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- Safety checks: required in-memory state from Section 8 ----\n",
    "if \"name2main\" not in globals() or not isinstance(name2main, dict):\n",
    "    raise RuntimeError(\"name2main not found. Run Section 8 (SIMBAD resolution) before running Section 8b.\")\n",
    "\n",
    "if \"key_norm\" not in globals() or \"safe_clean_name\" not in globals():\n",
    "    raise RuntimeError(\"Missing helpers (key_norm/safe_clean_name). Run Section 8 before running Section 8b.\")\n",
    "\n",
    "# alias2main is optional (may be empty if alias cache not provided)\n",
    "if \"alias2main\" not in globals() or not isinstance(alias2main, dict):\n",
    "    alias2main = {}\n",
    "    print(\"[INFO] alias2main not available; proceeding with name2main only.\")\n",
    "\n",
    "# 1) Load unresolved concept–object edges\n",
    "co_unres_path = RUN_DIR / \"concept_object_edges_unresolved_llm.csv.gz\"\n",
    "if not co_unres_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing unresolved edges file: {co_unres_path}\")\n",
    "\n",
    "co = pd.read_csv(\n",
    "    co_unres_path,\n",
    "    dtype={\"label\": \"int32\", \"object_name_norm\": \"string\", \"n_papers\": \"int32\", \"total_weight\": \"float32\"},\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# 2) Clean + key\n",
    "co[\"object_name_clean\"] = (\n",
    "    co[\"object_name_norm\"]\n",
    "    .astype(\"string\")\n",
    "    .fillna(\"\")\n",
    "    .map(lambda s: safe_clean_name(s))\n",
    ")\n",
    "co = co[co[\"object_name_clean\"].str.len() > 0].copy()\n",
    "co[\"object_key\"] = co[\"object_name_clean\"].map(key_norm)\n",
    "\n",
    "# 3) Map to SIMBAD main_id (object_id)\n",
    "def map_main_id(k: str) -> str | None:\n",
    "    mid = alias2main.get(k)\n",
    "    if isinstance(mid, str) and mid.strip():\n",
    "        return mid\n",
    "    mid = name2main.get(k)\n",
    "    if isinstance(mid, str) and mid.strip():\n",
    "        return mid\n",
    "    return None\n",
    "\n",
    "co[\"object_id\"] = co[\"object_key\"].map(map_main_id).astype(\"string\")\n",
    "\n",
    "n_total_rows = len(co)\n",
    "n_resolved_rows = int(co[\"object_id\"].notna().sum())\n",
    "\n",
    "print(\"Rows total:\", f\"{n_total_rows:,}\")\n",
    "print(\"Rows with resolved object_id:\", f\"{n_resolved_rows:,}\", f\"({n_resolved_rows/max(1,n_total_rows):.2%})\")\n",
    "\n",
    "# 4) Keep SIMBAD-only rows\n",
    "co_res = co.dropna(subset=[\"object_id\"]).copy()\n",
    "\n",
    "# 5) Aggregate by (label, object_id) to avoid duplicates from aliasing\n",
    "co_res_agg = (\n",
    "    co_res\n",
    "    .groupby([\"label\", \"object_id\"], as_index=False)\n",
    "    .agg(\n",
    "        n_papers=(\"n_papers\", \"sum\"),\n",
    "        total_weight=(\"total_weight\", \"sum\"),\n",
    "        raw_name_count=(\"object_name_clean\", \"nunique\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "co_res_agg[\"n_papers\"] = co_res_agg[\"n_papers\"].astype(\"int32\")\n",
    "co_res_agg[\"total_weight\"] = co_res_agg[\"total_weight\"].astype(\"float32\")\n",
    "co_res_agg[\"raw_name_count\"] = co_res_agg[\"raw_name_count\"].astype(\"int16\")\n",
    "\n",
    "print(\"Resolved aggregated edges:\", co_res_agg.shape)\n",
    "print(\"Unique objects:\", co_res_agg[\"object_id\"].nunique())\n",
    "\n",
    "# 6) Write resolved edges (renamed; no tier wording)\n",
    "OUT_RES = RUN_DIR / \"concept_object_edges_resolved_simbad_only_llm.csv.gz\"\n",
    "co_res_agg.to_csv(OUT_RES, index=False, compression=\"gzip\")\n",
    "print(\"Wrote:\", OUT_RES)\n",
    "\n",
    "# 7) Optional: noReg filtering if otype_by_main exists\n",
    "def is_region_like(main_id: str, otype: str | None) -> bool:\n",
    "    s = (main_id or \"\").upper()\n",
    "    t = (otype or \"\").upper()\n",
    "    # conservative: exclude SIMBAD object types containing REGION/FIELD\n",
    "    if \"REG\" in t or \"FIELD\" in t:\n",
    "        return True\n",
    "    # also exclude main IDs that are clearly NAME Field/Region\n",
    "    if s.startswith(\"NAME \") and (\"FIELD\" in s or \"REGION\" in s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if \"otype_by_main\" in globals() and isinstance(otype_by_main, dict) and len(otype_by_main) > 0:\n",
    "    mask_bad = co_res_agg[\"object_id\"].map(lambda mid: is_region_like(mid, otype_by_main.get(mid))).astype(bool)\n",
    "    co_res_noreg = co_res_agg[~mask_bad].copy()\n",
    "\n",
    "    OUT_NOREG = RUN_DIR / \"concept_object_edges_resolved_simbad_only_llm_noReg.csv.gz\"\n",
    "    co_res_noreg.to_csv(OUT_NOREG, index=False, compression=\"gzip\")\n",
    "    print(\"Wrote:\", OUT_NOREG, \"| dropped:\", int(mask_bad.sum()))\n",
    "else:\n",
    "    print(\"[SKIP] otype_by_main not available; skipping noReg output\")\n",
    "\n",
    "# 8) Optional: object catalog (object_id -> otype) if available\n",
    "if \"otype_by_main\" in globals() and isinstance(otype_by_main, dict) and len(otype_by_main) > 0:\n",
    "    obj_ids = sorted(set(co_res_agg[\"object_id\"].astype(\"string\").tolist()))\n",
    "    obj_df = pd.DataFrame({\n",
    "        \"object_id\": obj_ids,\n",
    "        \"otype\": [otype_by_main.get(mid) for mid in obj_ids],\n",
    "    })\n",
    "\n",
    "    OUT_OBJ = RUN_DIR / \"simbad_object_catalog_llm.csv.gz\"\n",
    "    obj_df.to_csv(OUT_OBJ, index=False, compression=\"gzip\")\n",
    "    print(\"Wrote:\", OUT_OBJ, \"| rows:\", len(obj_df))\n",
    "else:\n",
    "    print(\"[SKIP] otype_by_main not available; skipping object catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfce48",
   "metadata": {},
   "source": [
    "author note: the above shows 103,214 objects in the dataset instead of 100,560 is because our simbad resolution cache contains some objects not present in the final run. They are subsequently discarded in the matrix building phase and the effective object count is 100,560."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
